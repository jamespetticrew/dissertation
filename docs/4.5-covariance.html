<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="4.5 Covariance function | dissertation.knit" />
<meta property="og:type" content="book" />

<meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />




<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook.">

<title>4.5 Covariance function | dissertation.knit</title>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
/* show arrow before summary tag as in bootstrap
TODO: remove if boostrap in updated in html_document (rmarkdown#1485) */
details > summary {
  display: list-item;
  cursor: pointer;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<!--bookdown:toc:end-->
<!--bookdown:toc:start-->
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="covariance" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Covariance function</h2>
<p>The covariance between <span class="math inline">\(f(\mathbf{x})\)</span> and <span class="math inline">\(f(\mathbf{x&#39;})\)</span> is computed from the parameter vectors <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{x&#39;}\)</span>. A key assumption of Gaussian processes is that points that are close together in <span class="math inline">\(\mathbf{x}\)</span>-space should be close together in <span class="math inline">\(f(\mathbf{x})\)</span>-space, and that when predicting <span class="math inline">\(f(\mathbf{x^\dagger})\)</span>, then observed points close to <span class="math inline">\(\mathbf{x^\dagger}\)</span> more strongly influence the prediction than those that are far away. The presence of the <span class="math inline">\(\mathbf{x}-\mathbf{x}&#39;\)</span> term in correlation functions encapsulates this idea. Another key assumption is that some elements of <span class="math inline">\(\mathbf{x}\)</span> bear more influence on neighbouring points than others. and that information on this can be found from the data. The hyperparameters <span class="math inline">\(\mathbf{\gamma}\)</span> of the function <span class="math inline">\(c(\cdot, \cdot)\)</span> encapsulate this idea. The function <span class="math inline">\(c(\cdot, \cdot)\)</span> must
produce covariances matrices for <span class="math inline">\(f\)</span> that have several properties:</p>
<ul>
<li>Covariance matrices are symmetric in that for matrix <span class="math inline">\(\mathbf{A}\)</span>, <span class="math inline">\(\mathbf{A}=\mathbf{A}^{-1}\)</span>. Consequently the covariance function <span class="math inline">\(c(\cdot, \cdot)\)</span> must also be symmetric in that <span class="math inline">\(c(\mathbf{x}, \mathbf{x&#39;}) = \cdot(\mathbf{x&#39;}, \mathbf{x})\)</span>.</li>
<li>Covariance matrices are positive semidefinite (PSD), which means they have no negative eigenvalues. Hence <span class="math inline">\(c(\cdot,\cdot)\)</span> must produce covariance matrices which are PSD.</li>
<li>A covariance function that is a function of <span class="math inline">\(\mathbf{x} - \mathbf{x&#39;}\)</span> is stationary, in that the covariance between two points does not depend on where they are in <span class="math inline">\(\mathcal{X}\)</span> but only the distance between them.</li>
<li>Furthermore a covariance function that is a function of <span class="math inline">\(|\mathbf{x} - \mathbf{x&#39;}|\)</span> is isotropic.</li>
</ul>
<p>If a covariance function is stationary, then it can be expressed as a product of a correlation matrix <span class="math inline">\(\mathbf{R}\)</span> and a constant variance <span class="math inline">\(\sigma^2\)</span>. Table <a href="#tab:kernels"><strong>??</strong></a> shows the univariate version of the covariance functions available in DiceKriging <span class="citation">(<a href="#ref-dicekriging" role="doc-biblioref">Roustant, Ginsbourger, and Deville 2012</a>)</span>, or more appropriately correlation functions, as each is multiplied by <span class="math inline">\(\sigma\)</span> to get the required covariance. All of the covariance functions in Table <a href="#tab:kernels"><strong>??</strong></a> produce symmetric, PSD matrices, and they are all stationary and isotropic.</p>
<p>The Matérn correlation function with parameter <span class="math inline">\(\nu = \frac{5}{2}\)</span> is a popular choice and is the default option in both the DiceKriging and RobustGaSP <span class="citation">(<a href="#ref-rgasp_manual" role="doc-biblioref">Gu, Palomo, and Berger 2022</a>)</span> software packages. The <span class="math inline">\(\nu\)</span> parameter in the Matérn family controls the smoothness of the function. As <span class="math inline">\(\nu \rightarrow \infty\)</span>, the Matérn function begins to look more like the Gaussian covariance function, which is infinitely differentiable and can hence be unrealistically smooth. For <span class="math inline">\(\nu = \frac{5}{2}\)</span>, the process is mean square twice differentiable, which guarantees the existence of first and second moments, and hence predictive means and variances, which is desirable. Achieving the same level of smoothness with the squared exponential kernel would come at the price of correlations quickly going to zero as <span class="math inline">\(|\mathbf{x} - \mathbf{x&#39;}|\)</span> increases <span class="citation">(<a href="#ref-rgasp_manual" role="doc-biblioref">Gu, Palomo, and Berger 2022</a>)</span>, which may be unrealistic, and cause numerical issues in the parameter estimation routine. In this dissertation, the Matérn <span class="math inline">\(\frac{5}{2}\)</span> correlation function is used.</p>
<p>The correlation functions <span class="math inline">\(r(\cdot, \cdot)\)</span> in Table <a href="#tab:kernels"><strong>??</strong></a> take two scalar inputs and return a single scalar output<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a>. To extend this to compute correlations for multivariate <span class="math inline">\(\mathbf{x}\)</span> the most common approach is to take the product of the univariate correlations:</p>
<p><span class="math display" id="eq:mv-correlation">\[\begin{equation}
\tag{4.11}
\mathbf{c}(\mathbf{\mathbf{x}, \mathbf{x&#39;}}) = \sigma^2 \prod_{i=1}^p c(x_i, x&#39;_i)
\end{equation}\]</span></p>
<p>where <span class="math inline">\(p\)</span> is the number of elements of <span class="math inline">\(\mathbf{x}\)</span>. For each of element of <span class="math inline">\(\mathbf{x}\)</span> a separate <span class="math inline">\(\gamma\)</span> is learned, and that controls how influential that input parameter is on determining the value and the precision of the emulator prediction at other points, by inflating or shrinking the effective distance between the points in the relevant <span class="math inline">\(\mathbf{x}\)</span>-component. The <span class="math inline">\(\mathbf{\gamma}\)</span> are often called length-scale parameters. For predictions at points in <span class="math inline">\(\mathcal{X}\)</span> further away from the observed <span class="math inline">\(\mathbf{x^*}\)</span>, the predictive variance will tend to <span class="math inline">\(\sigma^2\)</span>, as the <span class="math inline">\((x-x&#39;)\)</span> terms in the correlation functions go to infinity, and as such the exponential terms in Table <a href="#tab:kernels"><strong>??</strong></a> tend to one. If <span class="math inline">\(\gamma\)</span> is changed from 1 to 0.5 for one of the components of <span class="math inline">\(\mathbf{x}\)</span>, the average amount of variation observed in a fixed interval along the component axis would double. Their is an interplay between the <span class="math inline">\(\mathbf{\gamma}\)</span> and <span class="math inline">\(\sigma\)</span>, The smaller the <span class="math inline">\(\mathbf{\gamma}\)</span> are the more the variation arises due to the function, and hence <span class="math inline">\(\sigma^2\)</span> would be smaller, as large fluctuations in <span class="math inline">\(f\)</span> can be explained by function variation rather than noise. Higher values for <span class="math inline">\(\mathbf{\gamma}\)</span> are associated with higher values for <span class="math inline">\(\sigma^2\)</span> and imply a constant function with a lot noise. Lower values for <span class="math inline">\(\mathbf{\gamma}\)</span> are associated with lower values for <span class="math inline">\(\sigma^2\)</span> and imply a white-noise process <span class="citation">(<a href="#ref-gp4ml" role="doc-biblioref">Williams and Rasmussen 2006</a>)</span>.</p>
<p>Often the function being emulated is non-deterministic. In this case, it is possible to learn another parameter from that data, called the ‘nugget’, which is the variance of a zero-centred Gaussian distribution. The univariate nugget is added to the diagonal elements of the covariance matrix for the observed data and the emulator is no longer constrained to interpolate the observed <span class="math inline">\(f(\mathcal{X^*})\)</span>. The nugget can also be used to address problems with inverting <span class="math inline">\(\mathbf{R}\)</span> in Equation <a href="4.3-model-selection.html#eq:likelihood">(4.4)</a> which can arise when its elements are very close to 0 or 1. In this dissertation a nugget parameter was used when training emulators to help with likelihood estimation.</p>
<p>There is a lot of flexibility in choosing covariance functions and they can be customised to reflect prior knowledge of the function being emulated. In particular, any sum or product of valid covariance functions is also a valid covariance function
<span class="citation">(<a href="#ref-gp4ml" role="doc-biblioref">Williams and Rasmussen 2006</a>)</span>. The covariance function is the defining element of a GP, but emulation is often boosted by the inclusion of a mean function, which is illustrates with a simple example, along with some other properties of the GP discussed in this section.</p>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-rgasp_manual" class="csl-entry">
Gu, Mengyang, Jesus Palomo, and James Berger. 2022. <em>RobustGaSP: Robust Gaussian Stochastic Process Emulation</em>. <a href="https://CRAN.R-project.org/package=RobustGaSP">https://CRAN.R-project.org/package=RobustGaSP</a>.
</div>
<div id="ref-dicekriging" class="csl-entry">
Roustant, Olivier, David Ginsbourger, and Yves Deville. 2012. <span>“<span class="nocase">DiceKriging, DiceOptim: Two R packages for the analysis of computer experiments by kriging-based metamodeling and optimization</span>.”</span> <em>Journal of Statistical Software</em> 51: 1–55.
</div>
<div id="ref-gp4ml" class="csl-entry">
Williams, Christopher KI, and Carl Edward Rasmussen. 2006. <em>Gaussian Processes for Machine Learning</em>. Vol. 2. 3. MIT press Cambridge, MA.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="8">
<li id="fn8"><p>A function that does this is called a kernel, and hence the <span class="math inline">\(r(\cdot, \cdot)\)</span> are sometimes called kernels.<a href="4.5-covariance.html#fnref8" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="4.4-conditioning-on-the-data.html"><button class="btn btn-default">Previous</button></a>
<a href="4.6-mean.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
