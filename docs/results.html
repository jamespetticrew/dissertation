<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Results | dissertation.knit</title>
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Results | dissertation.knit" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Results | dissertation.knit" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="GPR.html"/>
<link rel="next" href="Discussion.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">


      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="results" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">Chapter 5</span> Results<a href="results.html#results" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introduction-3" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> Introduction<a href="results.html#introduction-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section is described the history matching that was carried out using the simulator TALYS and experimental data from EXFOR as described in Chapter <a href="Background.html#Background">2</a>, following the methodology in described in Chapter <a href="History-Matching.html#History-Matching">3</a>, and emulating the simulator as described in Chapter <a href="GPR.html#GPR">4</a>. Three waves of history matching were carried out. The first wave and subsequent analysis is described in detail in Sections <a href="results.html#wave1-methodology">5.2</a>, <a href="results.html#wave1-analysis">5.3</a> and <a href="results.html#impl-optical-depth">5.4</a>. The results and analysis from subsequent waves are given in Sections <a href="results.html#subsequent-waves">5.5</a> and <a href="results.html#posterior-sampling">5.6</a>.</p>
</div>
<div id="wave1-methodology" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> First wave methodology<a href="results.html#wave1-methodology" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A total of 28 active input parameters were considered in the history matching analysis. The design grid for the first wave training runs were generated as follows. A 300 point Latin hypercube design was generated on U<span class="math inline">\([0,1]^{28}\)</span> using the function ‘maximinLHS’ from the R package ‘lhs’ <span class="citation">(<a href="#ref-lhs_r" role="doc-biblioref">Carnell 2022</a>)</span>. These points then underwent a linear transformation onto U<span class="math inline">\([-1,1]^{28}\)</span> and finally were exponentiated with base 10. This enabled exploration of the active parameter space both above and below the default parameter values (see Section <a href="Background.html#Background:talys">2.3</a>).</p>
<p>The simulator runs were carried out in parallel across 15 nodes on a personal laptop. The 300 simulator evaluations took approximately 36 hours to complete. The simulator produced cross-section spectra across the four reactions for which relevant experimental data were available, as shown in Table <a href="#tab:run-data"><strong>??</strong></a>, where it can be seen that six of the runs failed to complete successfully for the reactions (n,a), (n,n’) and (n,p) (see Section <a href="Background.html#Background:data">2.4</a> for an explanation of this notation).</p>
<p>Individual univariate Gaussian process emulators were built for each of the 588 TALYS outputs <span class="math inline">\(f\)</span> corresponding to the 588 experimental observations. A first order linear mean function was used for each emulator, which required learning 29 parameters from the data. Constant and 0 mean functions were also examined, but these led to an unacceptably large number of model failures arising from singular posterior correlation matrices. A Matern correlation function with smoothness parameter <span class="math inline">\(\frac{5}{2}\)</span> was used to aid the parameter estimation routine (Section <a href="GPR.html#model-selection">4.3</a>). The models were trained allowing a residual nugget to be estimated. This required learning a further 30 hyperparameters; 28 length-scale parameters, one variance and one residual nugget. None of the models failed to build. The inputs were transformed back to [-1,1] for training the models. The outputs were left unscaled in order to avoid potential errors when re-scaling the error terms for computing the implausibility measures.</p>
<p>A further set of 30 test runs were carried out, with the design matrix being generated using the same principles used to generate the training designs. The emulators were validated by predicting at the values of the inputs for the training runs and then computing the scaled Mahalanobis distances between the predicted outputs and the simulator outputs. If the assumptions of the Gaussian process emulator are valid, these distances are hypothesised to have a scaled-<span class="math inline">\(F_{m,n-q}\)</span> distribution <span class="citation">(<a href="#ref-gp_diagnostics" role="doc-biblioref">Bastos and O’Hagan 2009</a>)</span>, where <span class="math inline">\(m\)</span> is the number of test points (30), <span class="math inline">\(n\)</span> is the number of training points, which varies across models, and <span class="math inline">\(q\)</span> is the number of parameters in the emulator mean function (29). The distance metrics were compared to critical values corresponding to the one and 99 percentiles of the appropriate F distributions and the models were discarded if they lay outside this 98% probability interval. As a result of this, 2% of the valid emulators could be discarded on average, which is the price of protecting against the consequences of predicting with bad emulators. Of the 588 models, 124 were discarded this way, leaving 464.</p>
<p>Figure <a href="results.html#fig:dodgy-emulator">5.1</a> illustrates the importance of carrying out emulator evaluation. The Figure shows evaluation run results for an emulator for the simulated cross-section at 6.19 MeV for the reaction (n,p). This emulator was rejected at evaluation. The vertical lines show the two standard deviation predictive intervals for the emulator and the line x=y is plotted for reference. Also shown, at the intersection of the horizontal and vertical lines is the nominal measured cross-section value at 15.2 barns. The job of the emulator is to try and predict 15.2 barns at the same active parameter values as the simulator predicts 15.2 barns. From the plot, it appears that the emulator consistently under-predicts the simulator in this region. Consequently, the emulator would predict that the cross-section would be close to 15.2 barns in a region of parameter space where the simulated cross-section would be considerably higher. As a consequence of this, a non-implausible value of the active parameters could be rejected as implausible, and the opportunity to run the simulator at this input and better emulate close to the observed value could be lost. Hence, it is important to carry out validation on the emulators to ensure good out-of-sample predictive performance.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dodgy-emulator"></span>
<embed src="figures/dodgynp6-19.pdf" title="Validation run results for an emulator for the cross-section for the (n,p) reaction at energy 6.19 MeV. The vertical lines show the two standard deviation predictive intervals of the emulator and the x=y line has been plotted for reference. The observed cross section value is shown as the intersection of the vertical and horizontal line at (15.2,15.2). It appears that the emulator would underpredict the simulator output at this point. " width="80%" type="application/pdf" />
<p class="caption">
Figure 5.1: Validation run results for an emulator for the cross-section for the (n,p) reaction at energy 6.19 MeV. The vertical lines show the two standard deviation predictive intervals of the emulator and the x=y line has been plotted for reference. The observed cross section value is shown as the intersection of the vertical and horizontal line at (15.2,15.2). It appears that the emulator would underpredict the simulator output at this point.
</p>
</div>
</div>
<div id="wave1-analysis" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> First wave analysis<a href="results.html#wave1-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The computed scaled Mahalanobis distances for the accepted models for (n,tot) are plotted in Figure <a href="results.html#fig:mahalanobis">5.2</a>. The hypothesised scaled <span class="math inline">\(F_{30,271}\)</span> curve is plotted over the top. The distribution of distances should resemble this curve under Gaussian process emulator assumptions. The histogram and the curve appear to have reasonably similar densities, although the mode is shifted slightly right.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mahalanobis"></span>
<img src="dissertation_files/figure-html/mahalanobis-1.png" alt="Histogram of scaled Mahalanaobis distances for valid (n,tot) models. The hypothesised scaled-$F_{30,271}$ curve is plotted over the top. The distribution of Mahalanobis distances looks reasonably similar to the hypothesised distribution." width="80%" />
<p class="caption">
Figure 5.2: Histogram of scaled Mahalanaobis distances for valid (n,tot) models. The hypothesised scaled-<span class="math inline">\(F_{30,271}\)</span> curve is plotted over the top. The distribution of Mahalanobis distances looks reasonably similar to the hypothesised distribution.
</p>
</div>
<p>The 464 models were used to find non-implausible active parameter values for wave two. The number of potential points that could be evaluated was limited by memory and compute power. A total of 142860 points were selected at random from the 28 dimensional active parameter space were selected. Implausibility metrics were generated for each of the 464 models for each of the 142860 proposal points. Equations <a href="History-Matching.html#eq:one-d-implausibility">(3.1)</a> was evaluated to compute 464 one-dimensional implausibility measures for each proposal <span class="math inline">\(\mathbf{x}\)</span> and Equation <a href="History-Matching.html#eq:chi-sq-impl">(3.2)</a> was evaluated to compute one multidimensional implausibility metric for each proposal <span class="math inline">\(\mathbf{x}\)</span> using the corresponding experimental observations and uncertainties as described in Chapter <a href="History-Matching.html#History-Matching">3</a>.</p>
<p>Two criteria were used for accepting a proposal point <span class="math inline">\(\mathbf{x_j}\)</span>, First that <span class="math inline">\(I_j^{(uv)}&lt;3\)</span> and second that <span class="math inline">\(I^{(mv)}_j &lt; \chi^2_{464,.95}\)</span>. The second highest of the 464 implausibility measures for proposal <span class="math inline">\(\mathbf{x_j}\)</span>was chosen as <span class="math inline">\(I_j^{(uv)}\)</span>. 272 points met these criteria - 0.019% of the proposed points.</p>
<p>Different acceptance criteria for the two implausibility measures were examined; Table <a href="#tab:cutoffs"><strong>??</strong></a> shows some of the results of this analysis, where it can be seen that the choice which univariate implausibility measure to use was the most important decision in determining the size of the non-implausible active parameter space. Interestingly, choosing the highest univariate implausibility for each proposal resulted in zero non-implausible samples, and so no one sample was non-implausible for every process.</p>
</div>
<div id="impl-optical-depth" class="section level2 hasAnchor" number="5.4">
<h2><span class="header-section-number">5.4</span> Minimum implausibility and optical depth analysis<a href="results.html#impl-optical-depth" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The sensitivity of implausibility to the inputs can be visualised in plots such as those in Figure <a href="results.html#fig:implausibility-plots">5.3</a>. The left column shows minimum implausibilities and the right column shows plots of probabilities for two of the inputs. The minimum implausibility plots were generated by subdividing the <span class="math inline">\([-1,1]^2\)</span> grid (the log<span class="math inline">\(_{10}\)</span> of the parameter space) for two of the input parameters into 100 blocks. Each proposal input was then put into one of these intervals according to its values of the two relevant inputs. The minimum univariate implausibility measure over all the candidate <span class="math inline">\(\mathbf{x}\)</span>s from each interval was then pulled out and used to determine the hue in that interval in the heat map. The code used to generate these plots is shown in Appendix <a href="r-code-impl.html#r-code-impl">B</a>.</p>
<p>If the value of the minimum implausibility was high, this might indicate that it is not possible to get TALYS to match experimental measurements when run at that combination of values for those elements of <span class="math inline">\(\mathbf{x_j}\)</span>. The optical depth plots were constructed by grouping the proposal inputs in the same way. An estimate of the probability of finding a non-implausible input in an interval was then computed as the ratio of the number of non-implausible inputs to the total number of proposal inputs in that interval. Taken together, the plots help in visualising the sensitivity of TALYS to different values of the inputs.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:implausibility-plots"></span>
<img src="figures/impl_plot2.png" alt="Minimum implausibility plots (left column) and optical depth plots (right column) for six of the inputs. The minimum implausibilty plots show estimates of the minimum univariate implausibility for values of two of the inputs and the optical depth plots show estimates of the probability of finding a non-implausible input for values of two of the inputs. Note that the heatmaps are not all on the same scale." width="100%" height="200%" />
<p class="caption">
Figure 5.3: Minimum implausibility plots (left column) and optical depth plots (right column) for six of the inputs. The minimum implausibilty plots show estimates of the minimum univariate implausibility for values of two of the inputs and the optical depth plots show estimates of the probability of finding a non-implausible input for values of two of the inputs. Note that the heatmaps are not all on the same scale.
</p>
</div>
<p>The first row of Figure <a href="results.html#fig:implausibility-plots">5.3</a> shows implausibility and optical depth plots for parameters ‘d1adjust’ and ‘rvadjust’. These plots indicate that larger positive values for ‘rvadjust’ are more likely to cause TALYS to provide a poor match to observations, and that values below zero (that is multipliers below one, so values for ‘rvadjust’ less than the default) are more likely to produce non-implausible results. The optical depth plot indicates that there might be some interaction effect between ‘d1adjust’ and ‘rvadjust’, as indicated by the higher probability region in the top left corner of the plot. The second row of Figure <a href="results.html#fig:implausibility-plots">5.3</a> shows plots for parameters ‘awadjust’ and ‘rvsoadjust’. This plot also suggets that there may be an interaction between the two parameters, inputs with values of ‘awadjust’ close to -1 and ‘rvsoadjust’ close to +1 being more likely to produce non-implausible outputs. The bottom row shows plots for the parameters ‘rwadjust’ and ‘d3adjust’ and suggests that values of ‘rwadjust’ close to the TALYS default are more likely to give non-implausible outputs, and that perhaps the outputs are not very sensitive to the value for ‘d3adjust’.</p>
</div>
<div id="subsequent-waves" class="section level2 hasAnchor" number="5.5">
<h2><span class="header-section-number">5.5</span> Subsequent waves<a href="results.html#subsequent-waves" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The 276 non-implausible inputs chosen in Section <a href="results.html#wave1-analysis">5.3</a> were used as the design for the training runs in wave two. A new set of 588 emulators were built. A further set of test runs of the simulator were carried out. The active input parameter values for these test runs were chosen by generating a large number of proposal samples and evaluating their implausibility using the wave one emulators. The wave two emulators were then validated in the same way as described in Section <a href="results.html#wave1-methodology">5.2</a>. 163 were invalidated, leaving 425 emulator-measurement pairs with which to carry out implausibility analysis for wave 2. Less computational resource was available at the time of the wave two implausibility analysis, and consequently only 35715 proposal points were examined. Of these, 18043 were accepted as non-implausible, which is just over 50%. The cut-offs used for the implausibility measures were the same as those used in Section <a href="results.html#wave1-analysis">5.3</a>, although the relevant <span class="math inline">\(\chi^2\)</span> distribution from which the 99th percentile was taken was different, due to their being a different number of validated model between the waves (see Section <a href="History-Matching.html#HM-Multi-D">3.11</a>).</p>
<p>Having such a large number of non-implausible points for wave two presented a new challenge, as it was impossible to run TALYS for all 18043 non-implausible inputs in an acceptable time interval. Consequently a subset of the inputs had to be selected. It was decided to run the simulator at 600 training points for wave three (plus 60 test points). Instead of randomly sampling 600 of the 18043 non-implausible proposals, an attempt was made to find an optimal set of points by maximising the minimum distance between 28 input coordinates. This was carried out by first choosing a random sample of 600 points and computing their distance matrix (using the R function ‘dist’). The minimum value in the matrix was extracted. This process was repeated for <span class="math inline">\(10^5\)</span> random samples of 600. The sample that had the maximum minimum distance was used as the design for wave three, which was a way of attempting to select a set of design points that were as spread out across the non-implausible volume as possible.</p>
<p>The same workflow for building and validating the emulators was carried out for Wave 3, in which 440 emulators were assessed as valid. In wave three, different cut-offs were used for the implausibility measures, as the ability of the emulators to refocus with the wave one and two cut-offs was diminished. Table <a href="#tab:hm-summary"><strong>??</strong></a> shows a summary of the number of training runs and the values of the cut-offs used in each wave. The table also shows the proportion of the original input space evaluated as non-implausible after each run. This is computed as the proportion of proposal samples evaluated as non-implausible in the run multiplied by the proportion of the original space deemed non implausible in the previous run. For example, in wave teo, 18043/35715 = 0.505 of the runs were evaluated as non implausible. The proportion of non-implausible space from wave one was 0.19. The product of these proportions is 0.09. Implicitly, the proportion of the input space that is plausible before wave one (wave zero) is 1. From Table <a href="#tab:hm-summary"><strong>??</strong></a> it can be seen that it was possible to go from a state of complete ignorance about the values of the state parameters to a subset of values 0.03% the size of the original volume for the price of about 1300 TALYS runs.</p>
<p>In the third wave, 3572 proposal inputs were examined, and 1115 were evaluated as non-implausible, about 31%. This was the final wave, and consequently the 1115 were the final non-implausible inputs. These samples contained useful information about the relationship between the active parameters and the TALYS outputs and could also be used to allow sampling from the posterior distribution of the active parameters (conditional on the data), as discussed in the next Section.</p>
</div>
<div id="posterior-sampling" class="section level2 hasAnchor" number="5.6">
<h2><span class="header-section-number">5.6</span> Posterior sampling of active inputs<a href="results.html#posterior-sampling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Eventually the iterative history matching process must stop. This might be because there is enough evidence to suggest that further waves will not reduce the non-implausible input space much further, or that there is a suitable probability of choosing a non-implausible random sample from the remaining active parameter space, such as in <span class="citation">(<a href="#ref-jeremy_histmatch" role="doc-biblioref">Andrianakis et al. 2015</a>)</span>. In this dissertation the process was stopped as all the time/compute resource available was used up. Results that arise in this way can still be useful due to the rapid reduction of the plausible parameter space that occurs in early waves, as can be seen in Table <a href="#tab:hm-summary"><strong>??</strong></a>.</p>
<p>One endpoint of nuclear data evaluation is a set of multivariate normal distributions, one for each reaction, describing a suite of cross-sections on a fine energy grid. Consequently, the non-implausible active parameter space, denoted <span class="math inline">\(\{\mathbf{x_{ni}}\}\)</span>, is of secondary interest and the primary interest lies in the implication for the distribution of non-implausible TALYS outputs. One way to learn about the distribution of these non-implausible outputs would be through Monte Carlo (MC) sampling, where TALYS is run at samples of <span class="math inline">\(\mathbf{x}\)</span> drawn from the posterior distribution of active parameters. The results can be used to learn the properties of the required multivariate normal distributions. This requires being able to generate samples from the required posterior. The non-implausible parameter space is not an estimate of the posterior distribution of <span class="math inline">\(\mathbf{x}\)</span>, denoted <span class="math inline">\(\pi(\mathbf{x}|z)\)</span>, but it does contain it. Consequently it was possible to sample from it, using the following method, taken from <span class="citation">(<a href="#ref-jeremy_histmatch" role="doc-biblioref">Andrianakis et al. 2015</a>)</span>.</p>
<p>A proposal density was required to allow proposal posterior samples to be generated. A sensible choice was</p>
<p><span class="math display">\[\begin{equation}
\label{eq:proposal}
P(\mathbf{x}) = N\left(\mathbf{x}|\hat{\mathbf{\mu}}_x, \kappa\hat{\mathbf{\Sigma}}_x \right)
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\hat{\mathbf{\mu}}_x\)</span> and <span class="math inline">\(\hat{\mathbf{\Sigma}}_x\)</span> were the sample mean and covariance matrices of <span class="math inline">\(\{\mathbf{x_{ni}}\}\)</span>. Random samples were drawn from Equation <a href="#eq:proposal">(<strong>??</strong>)</a>, each one a proposal for a sample from <span class="math inline">\(\pi(\mathbf{x}|z)\)</span>. The constant <span class="math inline">\(\kappa\)</span> was used to ‘widen’ the search area, which has been seen to make the posterior sampling more effective <span class="citation">(<a href="#ref-jeremy_histmatch" role="doc-biblioref">Andrianakis et al. 2015</a>)</span>. The first step was to draw a large set of proposal samples from Equation <a href="#eq:proposal">(<strong>??</strong>)</a>. In this dissertation, this was done with the aid of the R package ‘mvtnorm’ <span class="citation">(<a href="#ref-mvtnorm" role="doc-biblioref">Genz et al. 2021</a>)</span>. <span class="math inline">\(\kappa\)</span> was set equal to 2.</p>
<p>The likelihood of the data <span class="math inline">\(\mathbf{z}\)</span> with respect to the proposal samples needed be computed for each proposal. As an approximation to the likelihood</p>
<p><span class="math display">\[\begin{equation}
\label{eq:likelihood-posterior}
L(\mathbf{x}) = p\left(\mathbf{z}| \mathbf{x}\right) = N\left( \mathbf{z}| E[g(\mathbf{x})], V(\mathbf{x})\right)
\end{equation}\]</span></p>
<p>was used.</p>
<p>Each time a proposal <span class="math inline">\(\mathbf{x}\)</span> was generated from <span class="math inline">\(P(\mathbf{x})\)</span>, the wave three emulator predictions at this point <span class="math inline">\(\mathbf{\hat{f}(x)}\)</span> were computed. The mean vector in Equation <a href="#eq:likelihood-posterior">(<strong>??</strong>)</a> was the vector of predictive emulator means at <span class="math inline">\(\mathbf{x}\)</span>, each element a prediction of an observation from <span class="math inline">\(\mathbf{z}\)</span>. The covariance matrix was constructed by putting the sum of the emulator predictor variance, the observation uncertainty and simulator inadequacy as the diagonals, and zeros for the off diagonals. The likelihood of the data was then computed using ‘mvtnorm’. This is the multivariate equivalent of using the ‘dnorm’ function from base R, that is the probability density, <span class="math inline">\(L(\mathbf{x})\)</span> with respect to Equation <a href="#eq:likelihood-posterior">(<strong>??</strong>)</a> was computed with respect to <span class="math inline">\(\mathbf{z}\)</span>.</p>
<p>The likelihood <span class="math inline">\(P(\mathbf{x})\)</span> was also computed with respect to <span class="math inline">\(\mathbf{x}\)</span>, and a weight <span class="math inline">\(w(x) = \frac{L(\mathbf{x})}{P(\mathbf{x})}\)</span> was assigned to <span class="math inline">\(\mathbf{x}\)</span>. <span class="math inline">\(L(\mathbf{x})\)</span> is a measure of the probability of observing the data given the proposal <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(P(\mathbf{x})\)</span> as the probability of having proposed <span class="math inline">\(\mathbf{x}\)</span> in the first place.</p>
<p>Once weights have been computed for all the proposal <span class="math inline">\(\mathbf{x}\)</span>, if the proposals are randomly sampled with respect to their weights, this is approximately equivalent to sampling from the posterior <span class="math inline">\(\pi(\mathbf{x}|\mathbf{z})\)</span>. The weighted sampling was achieved using the R function ‘sample’ with the ‘prob’ option set equal to the vector of weights.</p>
<p>This approach requires that the simulator inadequacy and observation uncertainty terms were normally distributed, an assumption that is commonly leveraged <span class="citation">(<a href="#ref-bower2010galaxy" role="doc-biblioref">Bower, Goldstein, and Vernon 2010</a>)</span>. Implemented in this way, the sampling method also requires that the prior distribution of <span class="math inline">\(\mathbf{x}\)</span> is constant over the likelihood domain. This assumption is often reasonable giving how small the non-implausible space is with respect to the original parameter space after history matching.</p>
<p>In this dissertation, implementing this method proved challenging. Evaluating <span class="math inline">\(L(\mathbf{x})\)</span> almost always led to values below machine precision, effectively zero. One possible reason for this is that the history matching stopped at wave three, and the non-implausible space was not yet small enough to allow the emulators to predict very well. Hence the likelihood of the data given the parameters was vanishingly small. It may have also been due to the number of relevant observations, 440 for wave three. Given that the probability density must integrate to one it was perhaps not unusual that many instantaneous values for the density function of a 440-dimensional Gaussian would be vanishingly small. Some investigation revealed that reducing the number of relevant data points did indeed result in non-zero results for <span class="math inline">\(L(\mathbf{x})\)</span>. Consequently it was decided to generate posterior samples of <span class="math inline">\(\mathbf{x}\)</span> with respect to a subset of the observations <span class="math inline">\(\mathbf{z}\)</span>. As the elements of <span class="math inline">\(\mathbf{z}\)</span> each corresponded to a different reaction (see Section <a href="Background.html#Background:data">2.4</a>), a natural way to subset the observations was by reaction. From Section <a href="GPR.html#GPR:GPR">4.2</a>, any subset of a multivariate normal distribution is itself normal, consequently any marginal likelihood of Equation <a href="GPR.html#eq:likelihood">(4.4)</a> is also normal, and it was possible follow the procedure to generate samples from the marginal posterior densities of the active parameters. This also provided an interesting way to investigate if different values of the parameters were better at reproducing different reactions.</p>
<p>This approach resulted in samples from the posterior being generated that were non-zero outside of the allowable range of values ([-1,1] on the log scale). This was addressed by implying a uniform prior on [-1,1] and 0 otherwise, which translates in practical terms to zero-weighting any posterior samples generated outside of the interval [-1,1].</p>
<p>Plots for the posterior samples generated in this way are shown in Figures <a href="results.html#fig:np-posterior">5.4</a> and <a href="results.html#fig:na-posterior">5.5</a>. The likelihood was evaluated with respect to the experimental data for these reactions for which there were valid emulators, that is for three data points for reaction (n,a) and fifty data points for reaction (n,p).</p>
The posterior samples are fairly uniform across most of the parameters, which may indicate that the two reaction types are fairly insensitive to the settings of most of the parameters, at least at the observed energy states, or that further waves of history matching may be needec in order to build better emulators and increase the ‘signal’ in the posterior sampling. However, in both figures, it is evident that lower values of rvadjust and rwdadjust are favoured, and this is particularly evident in Figure <a href="results.html#fig:na-posterior">5.5</a>. Since there is evidence of this in both figures, there is a good chance that lower values of these parameters are characteristics of the scenario being examined, neutrons incident upon Iron-56, rather than characteristic of a particular reaction. Contrastingly, the (n,a) reaction appears not to favour any particular values for the v3adjust parameter, whereas the (n,p) reaction appears to favour higher values, which could indicate that the parameter is of particular importance in modelling that reaction.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:np-posterior"></span>
<embed src="figures/np_posterior.pdf" title="Posterior samples for active input parameters when the approximate likelihood is evaluated with respect to (n,p) experimental data. This reaction appears to favour lower values for parameters 'rvadjust' and 'rwdadjust' and higher values for 'v3adjust'." width="80%" type="application/pdf" />
<p class="caption">
Figure 5.4: Posterior samples for active input parameters when the approximate likelihood is evaluated with respect to (n,p) experimental data. This reaction appears to favour lower values for parameters ‘rvadjust’ and ‘rwdadjust’ and higher values for ‘v3adjust’.
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:na-posterior"></span>
<embed src="figures/na_posterior.pdf" title="Posterior samples for active input parameters when the approximate likelihood is evaluated with respect to (n,a) experimental data. This reaction shows a preference for lower values of 'rvadjust' and 'rwdadjust'." width="80%" type="application/pdf" />
<p class="caption">
Figure 5.5: Posterior samples for active input parameters when the approximate likelihood is evaluated with respect to (n,a) experimental data. This reaction shows a preference for lower values of ‘rvadjust’ and ‘rwdadjust’.
</p>
</div>
</div>
<div id="conclusion-3" class="section level2 hasAnchor" number="5.7">
<h2><span class="header-section-number">5.7</span> Conclusion<a href="results.html#conclusion-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this chapter, three waves of history matching were carried out, which was seen to shrink the non-implausible active parameter space to 0.03% of its original size. Sensitivity of the simulator output to a subset of the active parameters was analysed. Samples were drawn from the marginal posterior distributions of the active parameters.</p>
<p>The next chapter ends the dissertation with a discussion on the analysis and on how it could be improved upon and extended.</p>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-jeremy_histmatch" class="csl-entry">
Andrianakis, Ioannis, Ian R Vernon, Nicky McCreesh, Trevelyan J McKinley, Jeremy E Oakley, Rebecca N Nsubuga, Michael Goldstein, and Richard G White. 2015. <span>“Bayesian History Matching of Complex Infectious Disease Models Using Emulation: A Tutorial and a Case Study on HIV in Uganda.”</span> <em>PLoS Computational Biology</em> 11 (1): e1003968.
</div>
<div id="ref-gp_diagnostics" class="csl-entry">
Bastos, Leonardo S, and Anthony O’Hagan. 2009. <span>“Diagnostics for Gaussian Process Emulators.”</span> <em>Technometrics</em> 51 (4): 425–38.
</div>
<div id="ref-bower2010galaxy" class="csl-entry">
Bower, Richard G, Michael Goldstein, and Ian Vernon. 2010. <span>“<span class="nocase">Galaxy formation: a Bayesian uncertainty analysis</span>.”</span> <em>Bayesian Analysis</em> 5 (4): 619–69.
</div>
<div id="ref-lhs_r" class="csl-entry">
Carnell, Rob. 2022. <em>Lhs: Latin Hypercube Samples</em>. <a href="https://CRAN.R-project.org/package=lhs">https://CRAN.R-project.org/package=lhs</a>.
</div>
<div id="ref-mvtnorm" class="csl-entry">
Genz, Alan, Frank Bretz, Tetsuhisa Miwa, Xuefei Mi, Friedrich Leisch, Fabian Scheipl, and Torsten Hothorn. 2021. <em><span class="nocase">mvtnorm</span>: Multivariate Normal and t Distributions</em>. <a href="https://CRAN.R-project.org/package=mvtnorm">https://CRAN.R-project.org/package=mvtnorm</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="GPR.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="Discussion.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/06-wave1_analysis.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["dissertation.pdf", "dissertation.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
