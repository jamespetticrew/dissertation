# History matching {#History-Matching}

## Introduction {#HM-Intro}

In this section history matching is described and relevant notation is developed. A description is given  of the various sources of uncertainty that need to be characterised in order to carry out the analysis. Finally the metrics used to help identify suitable values for active TALYS parameters are described. 

## Motivation

The motivation for using history matching was to find a subset or subsets of TALYS's 28-dimensional active parameter space which could give rise to acceptable matches between TALYS outputs, denoted $f$, and the true cross-sections which it attempts to simulate, denoted $y$. The term 'acceptable' here implies that an exact match is not expected. One reason for this is that $y$ cannot be measured perfectly, but there is a discrepancy between the measured cross-section, denoted $z$, and $y$. A second reason is that TALYS is not expected to perfectly simulate $y$. At third reason is that history matching requires $f$ to be evaluated a large number of times and consequently a statistical emulator $\hat{f}$ is used to approximate $f$ (see Section \@ref(GPR)) because it takes much less time to evaluate. The relationships between these four quantities are shown in Figure \@ref(fig:uncertainty-sketch). If the discrepancies between the quantities are well quantified (see Sections \@ref(obs-unc)-\@ref(em-var)), then there is enough information to compute some metric to decide if the match been TALYS and the true process is non-implausible (Section \@ref(HM-Implausibility)) for some value of the active input parameters.


```{r uncertainty-sketch, fig.cap="Both the experimental measurement $z$ and the simulator $f$ imperfectly capture the true process $y$. A further discrepancy occurs because the simulator itself must be imperfectly emulated by $f$.",  echo=FALSE}
knitr::include_graphics("~/Maths/Sheffield/dissertation/BookdownTemplate/Dissertation/figures/uncertanties_bigger.png")
```


History matching works by iteratively discounting values of the active input parameters as implausible give measurements and relevant uncertainties. New sets of emulators are built which focus on predicting well in the non-implausible active parameter space, known as 'refocussing'. In order to describe it in more detail, we now develop our mathematical notation further. 

## Notation {#Intro-notation}

In history matching there is a simulator with $p$ active input parameters, denoted by the p-vector $\mathbf{x} = [x^{(1)}, x^{(2)},...,x^{(p)}]$. In this application $p=28$. $\mathbf{x}$  can be any point in the 28-dimensional volume $\mathcal{X}$, which could be the set of all $\mathbf{x}$ that the simulator accepts as inputs, or a smaller subset of feasible inputs dictated by physical constraints or expert knowledge. In this application, $\mathcal{X}=[0.1,10]^{28}$ as described in Section \@ref(Background:talys). The simulator implements a mathematical model describing some physical process $y$. The physical process is observed imperfectly through some measurement process $z$. In general there is a vector of measurements, denoted by the n-vector $\mathbf{z} = [z_1,z_2,...,z_n]$, and the individual measurements are indexed as $z_i~i \in 1,2,...,n$. Each $z_i$ corresponds to a physical process $y_i$, denoted collectively as $\mathbf{y} = [y_1,y_2,...,y_n]$. In this application $n=588$ (see Section \@ref(Background:data)). The simulator simulates all of $\mathbf{y}$ in one code run at some value of $\mathbf{x}$. The simulated output vector is denoted $\mathbf{f}(\mathbf{x})= [f_1(\mathbf{x}), f_2(\mathbf{x}),...,f_n(\mathbf{x}) ]$. In the context of the discussion in Section \@ref(Background:data), each element of $\mathbf{f(x)}$ corresponds to a cross-section for a given reaction at a given energy from a single code run.

The imperfection of the experimental measurement process is represented by writing

\begin{equation}
z_i = y_i + \delta_i
\label{eq:observation-uncertainty}
\end{equation}

where $\delta_i$ is a univariate Gaussian random variable with expectation 0 and variance $V^{(obs)}_i$ representing our belief about the uncertainty of $y_i|z_i$. 

The imperfection of the simulator is represented by writing

\begin{equation}
f_i(\mathbf{x}) = y_i + \phi_i
\label{eq:model_inadequacy}
\end{equation}

where $\phi_i$ is a zero-centred univariate Gaussian random variable with variance $V_i^{(s)}$ used to represent out belief about the uncertainty of $y_i|f_i(\mathbf{x})$. It is assumed that $\phi_i$ is independent from $\mathbf{x}$.

The aim of history matching is to identify a subset or subsets of $\mathcal{X}$ for which the simulator could feasibly produce outputs consistent with the true process $y$. This requires comparing the simulator output to observations for $m$ proposal inputs, $\mathbf{x_1},\mathbf{x_2},...,\mathbf{x_m}$, indexed using $j = 1,2,...,m$, where each of the $\mathbf{x_j}$ is a p-vector with a proposed scalar value for each of the $p$ active parameters. Theoretically, if the models in TALYS were a perfect representation of reality, there would exist a $\mathbf{x_j}$ such that $f_i(\mathbf{x_j})= y_i~ \forall i$. All of the the elements of $\mathbf{x}$ are theoretically observable quantities, so in that context a perfect match would arise if they could all be set to their 'correct' value. A perfect match could also arise for some proposal $\mathbf{x}$ if TALYS were not a perfect representation of reality (getting the right answer for the wrong reasons). Regardless, a perfect match between $f$ and $y$ could not be detected, because $y$ is never observed directly.

In order to well explore $\mathcal{X}$ it is required that $m$ be very large. It is often the case that one run of the simulator can take several hours to complete, and as such it is not possible to examine a very large number of proposal $\mathbf{x_j}$ directly. To address this, $k<<m$ training runs of the simulator are used to build $n$ emulators, $\hat{f}_1(\cdot),\hat{f}_2(\cdot),...,\hat{f}_n(\cdot)$ one for each of the $f_1(\cdot),f_2(\cdot),...,f_n(\cdot)$. The $\hat{f}_i$ are all statistically independent. The set of $k$ training points used to train the emulators is denoted $\mathcal{X}^*$. 

The $\hat{f}_i$ can then be used to predict the simulator outputs $f_i$ for many samples from $\mathcal{X}$. The emulators will predict the simulator outputs at points from $\mathcal{X}^*$ exactly^[In Section \@ref(covariance) this idea is modified slightly, but the ideas discussed here still stand] but predictions at points not from $\mathcal{X}^*$ will be imprecise. In Chapter \@ref(GPR) it will be seen that the $\hat{f}_i$ are Gaussian random variables whose means and variances depend on $\mathbf{x}$. 

Our uncertainty about the simulator output at unobserved $\mathbf{x_j}$ is represented by writing

\begin{equation}
\label{eq:emulator-uncertainty}
f_i(\mathbf{x_j}) \sim \N\left(\E[f_i(\mathbf{x_j})],\V[f_i(\mathbf{x_j})] \right)
\end{equation}

where $\E[f_i(\mathbf{x_j})]$ and $\V[f_i(\mathbf{x_j})]$ are the mean and variance of emulator $\hat{f}_i(\mathbf{x_j})$.

A summary of the notation used hereafter is presented in Table \ref{tab:notation}.

\begin{table}\centering
\caption{A summary of notation used to describe the history matching analysis}
\label{tab:notation}
\begin{tabular}{|l|l|}
\hline
\textbf{Symbol}&\textbf{Meaning}\\
\hline
$\mathbf{x}$ or $\mathbf{x}'$& Vector of simulator calibration parameters\\
\hline
$p$& Number of active input parameters \\
\hline
$\mathcal{X}$&The set of all values that $\mathbf{x}$ can take \\
\hline
$y_i$& The $i$th physical process that the simulator attempts to reproduce\\
\hline
$z_i$& An experimental measurement of the process $y_i$\\
\hline
$n$& The total number of experimental measurements available in the\\
&analysis and the dimensionality of the simulator output\\
\hline
$f_i(\mathbf{x_j})$& Simulated value of the physical process $y_i$ for calibration input $\mathbf{x_j}$\\
\hline
$\hat{f}_i$& Emulator for $f_i$\\
\hline
$\delta_i$& Random variable representing our uncertainty about the\\ 
&discrepancy between the process $y_i$ and its measurement $z_i$\\
\hline
$V_i^{(obs)}$& Variance of $\delta_i$\\
\hline
$\phi_i$&Random variable representing our uncertainty about the\\
&discrepancy between the process $y_i$ and its simulated value $f_i(\mathbf{x})$\\
\hline
$\mathcal{X^*}$& Subset of $\mathcal{X}$ used to build emulators for $\mathbf{f}$\\
\hline
$\mathcal{X^\dagger}$& Subset of $\mathcal{X}$ at which to predict the behaviours of $\mathbf{f}$ using $\mathbf{\hat{f}}$\\
\hline
$k$& Number of points in the training set $\mathcal{X^*}$\\
\hline
$m$& Number of active parameters, the dimension of the input $\mathbf{x}$\\
\hline
$E[f_i(\mathbf{x_j})]$& Emulator expectation value for the $i$th simulator output\\ 
& at input $\mathbf{x_j}$\\
\hline
$V[f_i(\mathbf{x_j})]$& Marginal emulator variance for the $i$th simulator output\\ 
& at input $\mathbf{x_j}$\\
\hline
\end{tabular}
\end{table}

## Workflow

In this subsection an overview of the steps iterated over in history matching is given, following the schematic shown in Figure \@ref(fig:hm-workflow).

```{r hm-workflow, fig.cap="Schematic showing a typical history matching workflow. The process is iterative where each iteration is called a wave, and a wave ends either with refocussing the search space or stopping with the current set of non-implausible samples.", echo=FALSE}
knitr::include_graphics("~/Maths/Sheffield/dissertation/BookdownTemplate/Dissertation/figures/hm-workflow-vertical.png")
```


\textbf{Choose design points for Wave 1}: It was required that a great many candidate $\mathbf{x}$ be examined, but the number of times that TALYS could be run to evaluate these inputs directly was relatively small. Consequently  the limited runs available were used to build statistical emulators for TALYS. Initially, all samples from $\mathcal{X}$ were equally plausible, and hence emulators were required that well represent TALYS's behaviour over all of $\mathcal{X}$. To maximise the chance of achieving this, and to minimise the predictive variance of the emulators, a space-filling design was employed that provided coverage of $\mathcal{X}$ in some way that is optimised for the number of runs afforded. In this dissertation a Latin Hypercube (LH) design was used [@lhs_r] to choose the $\mathcal{X}^*$. This is a high-dimensional extension of the Latin square experimental design.

\textbf{Run simulator at design points and use results to build emulators}: TALYS was run at all points in $\mathcal{X}^*$ to obtain the $\mathbf{x}\rightarrow f(\mathbf{x})$ mappings needed to train the emulators. The results were consolidated and formatted to suit the input format of the Gaussian process software package used (see Section \@ref(gp-software)). When training the first wave of emulators, some of them failed  due to convergence problems in their numerical parameter estimation routines (see Section \@ref(model-selection)). This may have been because the active parameter space was very large at this stage, and some outputs were hard to emulate initially. One advantage of history matching is that the analysis was not invalidated by simply dropping these emulators.

\textbf{Validate models and discard invalid ones}: A small number of TALYS runs were carried out in order to test the out-of-sample predictive performance of the emulators. Any emulators not giving satisfactory out-of-sample performance were dropped. More details on the validation methodology used in this dissertation are given in Section \@ref(wave1-methodology).

\textbf{Generate a great many candidate} $\mathbf{x}$: The validated emulators were used to predict the TALYS output many times for many $\mathbf{x}$  at a fraction of the cost of running the simulator. One TALYS runs took between 30 and 360 minutes to complete; contrastingly the emulators evaluated in a fraction of a second. Consequently a great many samples for $\mathbf{x}$ were generated and  the emulators were used to generate predictions for each one.

\textbf{Compute implausibility measures}: As discussed in Section \@ref(HM-Implausibility), implausibility metrics were computed for each candidate $\mathbf{x}$ based on the emulator predictions, the corresponding measurements, and the uncertainties present in the system. This typically leads to a large reduction in the non-implausible parameter space [@jeremy_histmatch;@bower2010galaxy], based on the implausibility metrics exceeding some acceptable threshold.

\textbf{Discard implausible candidates}: The reduction in non-implausible parameter space is know as refocussing, and resulted in one of two outcomes at the end of each wave. Either the non-implausible samples (or a subset of them) were used as the training points $\mathcal{X}^*$  for the next set of TALYS runs, with which new emulators were built. This has the advantage of focussing the emulators in areas which have been shown to give acceptable matches to observations, leading to more precise emulation in these regions. A second outcome is that the process was stopped, and the non-implausible samples were accepted as the outcome of the analysis. Stopping can occur because the computational budget for TALYS runs has been reached, or because the proportional reduction in parameter space that occurred in this wave is not significantly different than those in previous waves, indicating that there is little to be gained in continuing.

History matching can be used as a way of helping to understand which parameter values are consistent with observations, which may be especially useful if the parameters represent observable quantities. It may also be used as a precursor to an uncertainty quantification study where, having found the values for the input parameters consistent with observations, it is desired to find the set of simulator outputs consistent with those input parameters. 

In order to decide if candidate $\mathbf{x}$ are implausible, the uncertainties that are present in the system needed to be modelled. This is discussed in the next sections.


## Observation uncertainty {#obs-unc}

The process of measurement resembles an aleatoric process, in that if one measurement $z_i^{(1)}$ of the process $y_i$ is taken, and then a second measurement $z_i^{(2)}$, in general $z_i^{(1)} \neq z_i^{(2)}$. The general assumption, though, is that the measurement process is imprecise, but not systematically wrong. This implies that if $q$ measurements $z_i^{(1)},z_i^{(2)},...,z_i^{(q)}$ were taken, as q gets larger $\frac{1}{q}\sum_{j=1}^q z_i^{(j)} \rightarrow y_i$ and that the variance of the estimator for $y_i$ is proportional to $\frac{1}{\sqrt{q}}$. In most practical cases, it is only possible to take one measurement $z_i$ and the scientist must provide their own estimate of the observation uncertainty, based on their knowledge of the experimental process.

The authors of [@Schnabel_2021] provided tools for extracting and contextualising the uncertainties that accompanied the experimental data used in this dissertation. However, the computational expense required to use the tools was too large to undertake. Consequently a simple approach was taken in order allow progress to be made, and it was decided that the observation uncertainty $V_i^{(obs)}$ from Equation \@ref(eq:observation-uncertainty) for $y_i$ should be $0.1z_i$. 


## Simulator inadequacy {#sim-ad}

A discrepancy between the TALYS output $f$ and the true process $y$, is expected to arise in at least two ways. First, TALYS implements some mathematical model, which is assumed to be an imperfect representation of reality. Second, the simulator itself may implement the mathematical model imperfectly. 

The objective of history matching is not to find the 'correct' input parameters for a given simulation, and the method does not require the inputs to be physically meaningful. A consequence of this is that history matching can give the right answer for the wrong reasons. For example, the mathematical model may be imperfect, but for certain values of its parameters it might give a good match to observations. Consequently, simulator inadequacy is difficult to quantify, and even more difficult to characterise in terms of contributions from different sources. One approach would be to consult an SME or experts to attempt to elicit a probability distribution representing their belief about how great a discrepancy could arise between TALYS and reality. In this dissertation, in order to focus on the methodology, a very simple approach was taken to uncertainty quantification, equating it to the corresponding measurement uncertainty of the process being simulated. Consequently $V_i^{(s)}=V_i^{(obs)}$ and in particular, $\phi_i$ is independent of $\mathbf{x}$ in this model.

## Emulator variance {#em-var}

As described in more detail in Section \@ref(GPR), an emulator $\hat{f}$ was used to predict the output of the simulator $f$ at unobserved $\mathbf{x}$ as it allowed more efficient exploration of $\mathcal{X}$. The price for this efficiency was the introduction of another source of uncertainty. This uncertainty depends on $\mathbf{x}$. More detail is given in Chapter \@ref(GPR).

## Other sources of uncertainty {#other-sources}

In this dissertation we considered uncertainties that arise from the imprecisions of the measurement process, the inability of TALYS to perfectly simulate cross-sections, and the imprecise predictions that arise from using an emulator to represent TALYS. In some applications, other sources of uncertainty may exist. For example, the application could required a simulator that uses Monte Carlo methods, and as such does not always give the same output when ran twice at the same input. This kind of uncertainty can be estimated from multiple simulator runs for the same $\mathbf{x}$, and computing the sample variance over these runs. Once all sources of uncertainty have been quantified, they are used to compute implausibility metrics to help assess if the simulator $f_i$ could give non-implausible matches to the true process $y_i$.

## One-dimensional implausibility {#HM-Implausibility}

A good implausibility metric should be a function of $\mathbf{x}$ and give extreme values if $f_i(\mathbf{x})$ is unlikely to give an acceptable match to $y_i$. Consequently the metric should be proportional to $|f_i(\mathbf{x}) - y_i|$. It should also take into account uncertainties arising from measurement, simulation and emulation. The greater the combined uncertainties in the system, the less sure we can be in branding values of $\mathbf{x}$ implausible. Consequently the metric should be inversely proportional to the combined uncertainties. The standard approach to combining uncertainties [@jeremy_histmatch] [@bower2010galaxy] is to assume that they are independent, and consequently that the variances are additive. This is the approach taken here, where the one-dimensional implausibility metric for $\mathbf{x_j}$ for process $y_i$ is defined as

\begin{equation}
(\#eq:one-d-implausibility)
I_j^{(i)} = \frac{|\E[f_i(\mathbf{x_j})] - z_i|}{\sqrt{V_i^{(obs)} + V_i^{(sim)} + \V[f_i(\mathbf{x_j})]}}.
\end{equation}

Equation \@ref(eq:one-d-implausibility) is proportional to the difference between the observed and simulated process, inversely proportional to the total uncertainty in the system, and depends on $\mathbf{x_j}$ as required. In order to decide what value of $I_j$ to use as the cut-off for deeming $\mathbf{x_j}$ plausible/ implausible, Pukelsheim's $3\sigma$ rule is leveraged, which states that for any continuous unimodal distribution, 95% of its probability mass lies within 3 standard deviations of its mean [@three_sigma]. One way to proceed would then be to reject all values of $\mathbf{x_j}$ for which $I_j > 3$ as implausible. Assuming that $|E[f_i(\mathbf{x})] - z_i|$ meets Pukelsheim's requirements, 95% of all non-implausible $\mathbf{x_j}$ should be retained on average, at the cost of losing 5% of non-implausible $\mathbf{x_j}$. In practice, this boundary can be moved to allow a more suitable number of candidate $\mathbf{x_j}$ to be accepted as not implausible if required. 

## Worked example {#hm-worked-example}

Suppose that the function $f(x) = x^2-5x$  represents a simulator with a single input $x$ and the interest lies in the output $f$, which attempts to simulate some process $y$. A measurement $z$ of $y$ has been made and found that $z=100$, with some uncertainty associated with it. The simulator is run at five different values for $x$ and these runs are used to build an emulator for $f$. The simulator output is then predicted at a great many points. Figure \@ref(fig:impl-plot) illustrates the scenario. The dots are the design point, which are interpolated by the emulators mean predictions across the range [-10,10]. The emulator predictor variance has been summed with the observation uncertainty and simulator inadequacy variances and the corresponding 3$\sigma$ intervals are shown in the Figure as lines above and below the mean prediction.

```{r,echo=FALSE,warning=FALSE,message=FALSE,include=FALSE}
all_x <- seq(from=-10,to=10,length.out=100)
all_y <- all_x^2 - 5*all_x
extend_x <- seq(from=-10,to=100,length.out=1000)
subset <- readr::read_rds("~/Maths/Sheffield/dissertation/BookdownTemplate/Dissertation/data/subset.rds")
data <- tibble::tibble(all_x,all_y) %>%
  dplyr::rename("x" = all_x, "y" = all_y)
mod <- RobustGaSP::rgasp(design = as.matrix(subset$x), response = subset$y,
                         zero.mean = "No",
                         nugget.est =F )
p <-  predict(mod, all_x, X = mod@X)
preds_mod <- tibble::tibble(mean = p$mean, lower95 = p$lower95,
                            upper95 = p$upper95, x= all_x) %>%
  dplyr::left_join(subset,by="x") %>% 
  dplyr::mutate(Model = "Constant mean")
mod_zero <- RobustGaSP::rgasp(design = as.matrix(subset$x), response = subset$y,
                              zero.mean = "Yes",
                              nugget.est = F)
p_zero <-  predict(mod_zero, all_x, X = mod_zero@X)
preds_mod_zero <- tibble::tibble(mean = p_zero$mean, lower95 = p_zero$lower95 - 0.1*p_zero$mean -10,
                                 upper95 = p_zero$upper95 + 0.1*p_zero$mean+10, x= all_x) %>%
  dplyr::left_join(subset,by="x") %>%
  dplyr::mutate(Model = "Zero-mean")#,
               # upper95 = upper95+0.1*mean, lower95=lower95+.1*mean)
preds_mod_zero <- preds_mod_zero %>% dplyr::mutate( "Implausibility"= abs(mean-100)/(.25*(upper95-lower95)) )
 
true_y <- 100
xstart <- preds_mod_zero %>% dplyr::filter(lower95 >=true_y) %>% dplyr::pull(x) %>% max()
xend <-   preds_mod_zero %>% dplyr::filter(upper95 >=true_y) %>% dplyr::pull(x) %>% max()
```



```{r, impl-plot,echo=FALSE,warning=FALSE,message=FALSE, fig.cap="The simulator f is a function of one input x. The simulator has been run at the five points shown, and an emulator is built using these five points. Its predictions are shown interpolating the design points. An observation has been made of z=100, showns as the dashed horizontal line. The uncertanties arising due to the emulator, simulator and act of observation are combined and the 3 sigma uncertainty intervals are plotted above and below the mean predictions. If the 3 sigma cutoff is used for for the implausibility measure, only those values of x shown intersecting the rectangle are non-implausible.",fig.width=7.5, fig.height=6.5}
preds_mod_zero%>% ggplot(aes(x=x,y = mean)) + 
  geom_rect(xmin=xstart, xmax=xend,ymin=-Inf,ymax=Inf,fill="#44FBA3",alpha=.08) +
  geom_line() +
  geom_line(aes(y=upper95), color="red") +
  geom_line(aes(y=lower95), color="red") +
  geom_point(aes(y=y),col="blue") +
  theme_bw() + ylab("f(x)") + geom_hline(yintercept = true_y, linetype="longdash") +
  ggplot2::theme(axis.text=ggplot2::element_text(face="bold", size=12),
                 axis.title=ggplot2::element_text(face="bold", size=13))
```

If the implausibility cut-off is set at 3 $\sigma$, values for $x$ that intersect the rectangle in the plot are non-implausible; roughly the interval [-8.99,-6.97]. The correct value is around -7.81. If the simulator were then run at a point in that rectangular region, the emulator variance, would shrink to zero at that point, and greatly reducing the emulator variance at neighbouring points (see Section \@ref(covariance)), reducing the width of the rectangle further.  However, simulator inadequacy and observation uncertainty would still remain and so the 'correct' input could not be found precisely. It is also worth noting that the non-implausible region need not be continuous. From Figure \@ref(fig:impl-plot) it could be imagined that were the domain of $x$ to extend to +15, the mean prediction would intersect $f=100$ again, and indeed a second, much wider interval in $x$ would be deemed non-implausible as the 3 $\sigma$ intervals grow wider as the emulator predicts further from the data. In subsequent runs, candidate $x$'s from this interval would be evaluated using the simulator and used to build emulators that predict better in this region, consequently the non-implausible space would shrink. 

Equation \@ref(eq:one-d-implausibility) is a one-dimensional implausibility metric, computed with respect to a single process $y_i$. In this dissertation, 588 processes were examined simultaneously, and ideally the simulator can give non-implausible matches to them all for some value or values of $\mathbf{x_j}$. Consequently the idea of the implausibility measure in Equation \@ref(eq:one-d-implausibility) is extended to  compute an implausibility measure for $\mathbf{x_j}$ with respect to multiple processes $y_1,y_2,...,y_n$.

## Multidimensional implausibility {#HM-Multi-D}

588 outputs $f_1(\mathbf{x_j}),f_2(\mathbf{x_j}),...,f_{588}(\mathbf{x_j})$ were produced each time TALYS was run for some $\mathbf{x_j}$. Each output $f_i(\mathbf{x})$ had a corresponding measurement $z_i$ of a true process $y_i$. In the literature there exist some methods for emulating multivariate outputs and accounting for correlations [@jonty_efficient; @multivariate_gp] amongst the $f$s. In this dissertation, the simple approach of emulating each of the $f$ independently was taken. As a consequence of this, there were up to 588 univariate implausibility measures for each input, and the criterion for labelling $\mathbf{x_j}$ plausible or non-implausible had to  be modified. One approach was to take the maximum of the 588 measures as $I_j$ and to require that this be less than three for $\mathbf{x_j}$ to non-implausible. A common approach [@jeremy_histmatch; @bower2010galaxy] is to take the second or third largest $I^{(i)}_j$. The choice is a pragmatic one and the  sensitivity of the size of the non-implausible space to the choice of metric was examined in this dissertation, allowing a suitable choice can be made.

A second multivariate implausibility metric is

\begin{equation}
(\#eq:chi-sq-impl)
I(\mathbf{x_j}) = (\mathbf{z} - \E[\mathbf{f(x_j)}])^T\left(\mathbf{V}_i^{(obs)} + \mathbf{V}_i^{(sim)} + \mathbf{V}[\mathbf{f(x_j)}]\right)(\mathbf{z} - \E[\mathbf{f(x_j})])
\end{equation}

in which the vector of observations $\mathbf{z} = (z_1,z_2,...,z_n)$ and the corresponding emulated quantities $\mathbf{f(x_j)} = (f_1(\mathbf{x_j}),f_2(\mathbf{x_j}),...,f_n(\mathbf{x_j}))$ are used and where $\left(\mathbf{V}_i^{(obs)} + \mathbf{V}_i^{(sim)} + \mathbf{V}[f_i(\mathbf{x_j})]\right)$ should now be a full $n \times n$ covariance matrix. Under suitable assumptions, $I(\mathbf{x_j})$ has an asymptotic $\chi^2$ distribution and consequently an appropriate percentile can be chosen from the $\chi^2$ distribution with $n$ degrees of freedom and use as the implausible/ non--implausible cut-off for $\mathbf{x_j}$.

One potential problem could arise in specifying the full covariance structure of $\left(\mathbf{V}_i^{(obs)} + \mathbf{V}_i^{(sim)} + \mathbf{V}[\mathbf{f(x_j)}]\right)$. The simple approach was taken here to assume that all of the uncertainties were uncorrelated. In that case, all three covariance matrices are diagonal, with the $i,i$th element equal to the sum of the univariate uncertainties for that process, and all other elements zero. Taking this approach, it can be see that the multivariate implausibility measure from Equation \@ref(eq:chi-sq-impl) is just the sum of the squared univariate implausibility measures from Equation \@ref(eq:one-d-implausibility), $I_j(\mathbf{x_j}) = \sum_{i=1}^n \left(I_j^{(i)}\right)^2$

Here, $I_j^{(uv)}$ is used to denote the univariate implausibility metric chosen for a given wave, and $I_j^{(mv)}$ for the multivariate measure, where it is hoped that the intended meaning should be clear from the superscript, and that these would not be mistaken for the $uv$th or $mv$th univariate implausibility metric for $\mathbf{x_j}$.

## Advantages of history matching

History matching provides an efficient, pragmatic alternative to common inference methods such as maximum likelihood (ML) or Markov chain Monte Carlo (MCMC), which seek to learn a full distributional description of the active TALYS inputs from the data. In ML, the joint density of the inputs is modelled with some common likelihood function, commonly Gaussian. An optimisation algorithm is used to maximise this likelihood of the data with respect to the statistical model parameters. The maximum likelihood model parameters and the assumptions of the models together are sufficient to give a full probabilistic description of the active inputs. This method relies on the validity of the modelling assumptions and the success of the parameter estimation routine. Gradient-based, local optimisation algorithms risk converging on local, non-global maxima when trying to maximise the likelihood function, and this risk grows with dimensionality of the search space. Global optimisation algorithms can grow computationally very expensive and have no guarantee of successfully finding a global maximum for a finite run-time. In MCMC, the posterior distribution of the active inputs is not constrained to belong to a particular model family, instead samples can be numerically generated from the posteriors once the chains have converged on their joint stationary distribution. However, this convergence often requires a great many more samples than ML, and the chains can get stuck in regions of search space, both problems which can become worse in high dimensions.

Contrastingly, instead of searching for values of the active inputs that are most consistent with all of the observed data points, history matching identifies values of the active inputs that are inconsistent with one or more of the observed data points and discards them as implausible. This approach helps address some of the issues associated with high-dimensionality, and can rapidly reduce the non-implausible search space very quickly. The method is efficient because it is primarily concerned with simple functions of means and variances, which level of statistical detail is often appropriate given the performance of the simulators [@jeremy_histmatch]. History matching does not require any likelihoods to be defined or any complex probabilistic calculations. The method is also efficient because the non-implausible search space is identified in iterations known as waves, where the search space shrinks each time. Because it is not a formal inference method, pragmatic decisions can be made on things such as implausibility cut-offs and how many samples to take.

## Conclusion

In this section the process workflow of history matching was discussed, along with two of its important components, uncertainties and implausibility metrics. The idea of using a Gaussian process emulator as a fast approximation for TALYS was also discussed, enabling the active parameter space to be explored more efficiently, at the cost of introducing another source of uncertainty. Gaussian processes are discussed further in the next chapter. 

