# Results {#results}
```{r}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```



```{r, data import, include=FALSE,echo=FALSE}
library(magrittr)
# Pull out 5 < E < 10 MeV
fe_exp <- tibble::as_tibble(
  readRDS("~/Maths/Sheffield/dissertation/pipeline/01/expDt.rda") ) %>%
  dplyr::select(c("REAC","L1","DATA")) %>%
  dplyr::rename("Energy" = "L1", "Cross-section" = "DATA")%>%
  dplyr::filter(Energy>=5. & Energy<=10.) 
fe_exp$REAC <- dplyr::recode_factor(
  fe_exp$REAC,
  "(26-FE-56(N,A)24-CR-53,,SIG)" = "(n,a)",
  "(26-FE-56(N,INL)26-FE-56,,SIG)" = "(n,n')",
  "(26-FE-56(N,P)25-MN-56,,SIG)" = "(n,p)",
  "(26-FE-56(N,TOT),,SIG)" = "(n,tot)",
  "(26-FE-56(N,EL)26-FE-56,,SIG)" = "(n,n)")

lhs_samples <- readr::read_csv("~/Maths/Sheffield/dissertation/exp_design/lhs_samples.csv")
run_data <- readr::read_csv("~/Maths/Sheffield/dissertation/data/train_data_1_logscale.csv")%>%
  tidyr::drop_na()

mahals <- readr::read_rds("~/Maths/Sheffield/dissertation/models/wave1/R/rData/mahal_distances.rds")
chi_df<- readr::read_rds("~/Maths/Sheffield/dissertation/models/wave1/R/rData/chi_sq_df.rds")
plausibles <- readr::read_rds("~/Maths/Sheffield/dissertation/models/wave1/R/rData/plausibles.rds")

na <- readr::read_rds("~/Maths/Sheffield/dissertation/BookdownTemplate/Dissertation/data/na_posterior.rds")
np <- readr::read_rds("~/Maths/Sheffield/dissertation/BookdownTemplate/Dissertation/data/np_posterior.rds")
```


## First wave methodology {#wave1-methodology}

A total of 28 active input parameters were considered. The design grid for the first wave training runs were generated as follows. A 300 point maxmin Latin hypercube design was generated on $U[0,1]^{28}$. These points then underwent a linear transformation onto $U[-1,1]^{28}$ and finally were exponentiated with base ten. This enabled exploration of the design space both above and below the default parameter values. 

The simulator runs were carried out in parallel across 15 nodes on a personal laptop. The 300 simulator evaluations took approximately 36 hours to complete. The simulator produced cross-section spectra across the four of the reactions for which relevant experimental data existed, as shown in Table 2, where it can be seen that six of the runs failed to complete successfully for the reactions (n,a), (n,n') and (n,p) (see Section \@ref(Background:data) for an explanation of this notation). 

```{r run-data, fig.cap="Counts of succesful simulator runs for the four reactions for which relevant experimental data exist. Four of the runs failed for partial cross-sections ((n,a), (n,n'), (n,p) ) but still produced total cross-section results."}
run_data %>% tidyr::drop_na() %>% dplyr::group_by(Reaction) %>% 
  dplyr::summarise(count = dplyr::n()) %>%
  dplyr::filter(Reaction %in% c("(n,tot)", "(n,n')", "(n,p)", "(n,a)" )) %>%
  knitr::kable(format="latex",caption="Counts of cross-section spectra generated by TALYS across 300 runs. Not all simulator runs were succesful, but this does not prevent history matching from being carried out." ) 
```

Individual univariate Gaussian process emulators were built for each of the 593 energy points for
each of the four reactions in common between the observed and simulated cross-section data sets, 2372 models in all. This approach was inefficient as there did not exist observations at each of the 593 energy points across all four reactions, and much compute was wasted training and evaluating models for which implausibility measures could not be computed. This was addressed in subsequent waves. The emulators used a first order linear mean function, which required learning 29 parameters from the data. Constant and 0 mean functions were also examined, but these led to an unacceptably large number of model failures arising from singular posterior correlation matrices. A Matern correlation function with smoothness parameter $\frac{5}{2}$ was used. The models were trained allowing a residual nugget to be estimated. This required learning a further 30 hyperparameters; 28 length-scale parameters, one variance and one residual nugget. None of the models failed to build. The inputs were transformed back to [-1,1]. The outputs were left unscaled in order to avoid potential errors when scaling the error terms for the implausibility measures.

A further set of 30 test runs were carried out, with the design matrix being generated using the same principles used to generate the training designs. The emulators were validated by predicting at the values of the inputs for the training runs and then computing the Mahalanobis distances between the predicted outputs and the simulator outputs. If the assumptions of the Gaussian process emulator are valid, these distances are hypothesised to have a scaled-$F_{m,n-q}$ distribution, where $m$ is the number of test points (30), $n$ is the number of training points, which varies across models, and $q$ is the number of parameters in the emulator mean function (29). The distance metrics were compared to critical values corresponding to the one and 99 percentiles of the appropriate F distributions and the models were discarded if they lay outside this 98% probability interval. Of the 2372 models, 739 were discarded this way, leaving 1633.

Figure \@ref(dodgy-emulator) illustrates the importance of carrying out evaluation. The Figure shows evaluation run results for an emulator for the simulated cross-section at 6.19 MeV for the reaction (n,p). The vertical lines show the two standard deviation predictive intervals for the emulator and the line x=y is plotted for reference. Also shown, at the intersection of the horizontal and vertical lines is the nominal measured cross-section value at 15.2 barns. The job of the emulator is to predict 15.2 barns at the same active parameter values as the simulator. From the plot, it appears that, if the emulator consistently under-predicts the emulator in this region and if that pattern continued, the emulator would predict that the cross-section would be close to 15.2 barns in a region of parameter space where the simulated cross-section would be considerably higher. As a consequence of this, a non-implausible value of the active parameters could be rejected as implausible, and the opportunity to run the simulator at this input and better emulate close to the observed value could be lost. Consequently, it is important to carry out validation on the emulators to ensure good out-of-sample predictive performance.

```{r dodgy-emulator, fig.cap="Validation run results for an emulator for the cross-section for the (n,p) reaction at energy 6.19 MeV. The vertical lines show the two standard deviation predictive intervals of the emulator and the x=y line has been plotted for reference. The observed cross section value is shown as the intersection of the vertical and horizontal line at (15.2,15.2). It appears that the emulator would underpredict the simulator output at this point. "}
knitr::include_graphics("~/Maths/Sheffield/dissertation/BookdownTemplate/Dissertation/figures/dodgynp6-19.pdf")
```


## First wave analysis {#wave1-analysis}

The computed Mahalanobis distances for the accepted models for (n,tot) are plotted in Figure \ref(fig:mahalanobis). The hypothesised scaled $F_{30,271}$ curve is plotted over the top. The distribution of distances should resemble this curve under Gaussian process emulator assumptions. The histogram and the curve appear to have reasonably similar densities, although the mode is shifted slightly right. 

```{r mahalanobis, fig.cap= "Histogram of Mahalanaobis distances for valid (n,tot) models. The hypothesised scaled-$F_{30,271}$ curve is plotted over the top. The distribution of Mahalanobis distances looks reasonably similar to the hypothesised distribution."}
mahals_sub <- mahals %>%
  dplyr::filter(reaction!="(n,g)")

valid_models <- mahals_sub %>%
  dplyr::filter(prob > .01 & prob < .99) 
mahals_sub %>%  dplyr::filter(reaction == "(n,tot)") %>%
  dplyr::pull(scaled) %>% hist(freq=F,
                                 ylim=c(0,3), main=NULL) 
x <- seq(0,3,length.out = 100)
m <- 30
n <- 300
q <- 29

y <- df(x, df1 = m, df2 = (n-q))
lines(x,y)
```

Of the 1633 validated models, 464 had corresponding observations allowing for implausibility analysis. Further waves trained models only at energy/ reaction combinations for which observations existed. The 464 models were used to find non-implausible design points for Wave 2. The number of potential design points that could be evaluated was limited by memory and compute power. A total of 142860 random points in the 28 dimensional design space were selected. Implausibility metrics were generated for each of the 464 models for each of the 142860 proposal points. In the implausibility analysis the assumption was made that observation uncertainty, $\sqrt{V_j^{(obs)}}$ for observation $z_j$ was $0.1z_j$ and that the simulator inadequacy, $\sqrt{V^{(s)}_j}$  for simulator output $f_j(\mathbf{x_i})$ was also $0.1z_j$, where $j \in 1,2,...,464$ indexes over the observations/ emulator pairs and $i \in 1,2,...,142860$ indexes over the proposal points, implying that the simulator inadequacy is independent of the design point at which the simulator is evaluated. The third type of uncertainty considered was emulator uncertainty, $\sqrt{V[f_j(\mathbf{x_i})]}$ corresponding to mean emulator output $E[(f_j(\mathbf{x_i})]$. Hence the implausibility measure for proposal point $i$ computed using simulator $j$ is 

\begin{equation}
\label{eq:implausibility}
I_{i,j} = \frac{ | z_j - E[f_j(\mathbf{x_i})]  |   }{ \sqrt{(V_j^{(obs)} + V^{(s)}_j + V[f_i(\mathbf{x_j})]   )   }}.
\end{equation}

Equation (1) was evaluated 464 times for each proposal point. The second largest of the 464 measures $I_i^{(2M)}$ for a proposal point $\mathbf{x_i}$ was used as the first implausibility metric for the proposal point. Another multivariate implausibility was also considered:

\begin{equation}
\label{eq:implausibility2}
I_{i} = \left( \mathbf{z} - E[f(\mathbf{x_i})] \right)^T \left(V_j^{(obs)} + V^{(s)}_j + V[f_i(\mathbf{x_j})] \right)^{-1} \left( \mathbf{z} - E[f(\mathbf{x_i})] \right)
\end{equation}

where $\mathbf{z}$ and $E[f(\mathbf{x_i})]$ are j-vectors with the 464 observations and their corresponding emulator predictions respectively, and $V_j^{(obs)}$, $V^{(s)}_j$ and $V[f_i(\mathbf{x_j})]$ are now all covariances matrices. The simple approach was taken in assuming that the outputs are all uncorrelated. In this case, all three covariance matrices are diagonal, with the square of the denominator in Equation (1) making the $j,j$-th element of the matrix. Taking this approach, $I_i$ for proposal point $i$ is simply $\sum_{j=1}^{464} I^2_{i,j}$.

Two criteria were used for accepting a proposal point $\mathbf{x_i}$, First that $I_i^{(2M)}<3$ and second that $I_i < \chi^2_{464,.95}$. 272 points met these criteria - 0.019% of the proposed points. The decision on how many points to examine, and what proportion of these to accept, was driven chiefly by practical considerations. The lists of implausibility measures for all the proposal points took up 5.4GB in memory, bringing an 8GB laptop close to capacity, and based on the 36 hour run time of the 300 wave one runs, a similar number of accepted points was desired to allow the analysis to be carried out in a reasonable amount of time. Given the size of the input space it would be desirable to generate a much larger proposal sample if resources allowed. Different acceptance criteria for the two implausibility measures were examined; Table 3 shows some of the results of this analysis, where it can be seen that the choice of maximal implausibility was the most important decision in determining the size of the non-implausible design space. 

\begin{table}\centering
\caption{Number of proposal points accepted as a function of different cutoffs for implausibility metrics. $I^{nM}$ indicates the $n$th largest univariate implausibility measure for that proposal point - the number of chosen points is more sensitive to the choice for this metric than to the choice of chi-squared cut-off percentile.}
\begin{tabular}{cccc}\\
&$I_i^{(1M)}$&$I_i^{(2M)}$&$I_i^{(3M)}$\\
\hline
$\chi^2_{464,.95}$& 0 & 268 & 4381 \\
$\chi^2_{464,.99}$& 0 & 276  & 4472 \\
\hline
\end{tabular}
\end{table}

## Minimum implausibility and optical depth analysis {#impl-optical-depth}

Some sense of the sensitivity of implausibility to the inputs can be gained from examining plots such as those in Figure \@ref(fig:implausibility-plots). The left column shows minimum implausibilities and the right column shows plots of probabilities for two of the inputs. The minimum implausibility plots were generated by subdividing the $[-1,1]^2$ grid for two of the input parameters into 100 blocks. Each proposal input was then put into one of these intervals according to its values of the two relevant inputs. The minimum univariate implausibility measure (Equation \@ref(eq:one-d-implausibility) in each interval was then pulled out and used to determine the hue in that interval in the heat map. If the value of the minimum implausibility was high, this might indicate that it is not possible to get TALYS to match experimental measurements if the input parameters are set to those values. The optical depth plots are constructed by grouping the proposal inputs in the same way. An estimate of the probability of finding a non-implausible input in an interval is then computed as the ratio of the number of non-implausible inputs to the total number of proposal inputs in that interval. Taken together, the plots help in visualising the sensitivity of TALYS to different values of the inputs.

The code used to generate these plots is shown in Appendix \@ref(r-code-impl).


```{r implausibility-plots,fig.cap="Minimum implausibility plots (left column) and optical depth plots (right column) for six of the inputs. The minimum implausibilty plots show estimates of the minimum univariate implausibility for values of two of the inputs and the optical depth plots show estimates of the probability of finding a non-implausible input for values of two of the inputs. Note that the heatmaps are not all on the same scale.", fig.height=20}
knitr::include_graphics("~/Maths/Sheffield/dissertation/BookdownTemplate/Dissertation/figures/impl_optical_depth.pdf")
```


The first row of Figure \@ref(fig:implausibility-plots) shows implausibility and optical depth plots for parameters 'd1adjust' and 'rvadjust'. These plots indicate that larger positive values for 'rvadjust' are more likely to cause TALYS to provide a poor match to observations, and that values below zero (that is multipliers below one, so values for 'rvadjust' less than the default) are more likely to produce non-implausible results. The optical depth plot indicates that there might be some interaction effect between 'd1adjust' and 'rvadjust', as indicated by the higher probability region in the top left corner of the plot. The second row of the figure shows plots for parameters 'awadjust' and 'rvsoadjust'. This plot also suggets that there may be an interaction between the two parameters, inputs with values of 'awadjust' close to -1 and 'rvsoadjust' close to +1 being more likely to produce non-implausible outputs. The bottom row shows plots for the parameters 'rwadjust' and 'd3adjust' and suggests that values of 'rwadjust' close to the TALYS default are more likely to give non-implausible outputs, and that perhaps the outputs are not very sensitive to the value for 'd3adjust'. 

## Subsequent waves {#subsequent-waves}

The 276 non-implausible inputs chosen n Section \@ref(wave1-results) were used as the design for the training runs in Wave 2. 593 emulators were built, one for each experimental data point. A further set of test runs of the simulator were carried out. The active input variable settings for these test runs were also chosen by generating a large number of proposal samples and evaluating their implausibility using the Wave 1 emulators. The emulators were then validated in the same way as described in Section \@ref(wave1-procedure). 168 were discarded, leaving 425 emulator-measurement pairs with which to carry out implausibility analysis for Wave 2. Less computational resource was available at the time of the Wave 2 implausibility analysis, and consequently only 35715 proposal points could be examined. Of these, 18043, which is just over 50%. The cut-offs used for the implausibility measures were the same as those used in Section \@ref(wave1-results), although the relevant $\chi^2$ distribution from which the 99th percentile was taken was different, due to their being a different number of validated model between the waves (see Section \@ref(HM-Multi-D).

Having such a large number of non-implausible points for Wave 2 presented a new challenge, as it was impossible to run TALYS for all 18043 non-implausible inputs in an acceptable time interval. Consequently a subset of the inputs had to be selected. It was decided to run the simulator at 600 training points for Wave 3 (plus 60 test points). Instead of randomly sampling 600 of the 18043 non-implausible proposals, an attempt was made to find an optimal set of points by maximising the minimum distance between 28 input coordinates. This was carried out by first choosing a random sample of 600 points and computing their distance matrix (using the R function 'dist'). The minimum value in the matrix was extracted. This process was repeated for $10^5$ random samples of 600. The sample that had the maximum minimum distance was used as the design for Wave 3, which was a way of ensuring that the design points were as spread out across the non-implausible volume as possible. 

The same for building and validating the emulators was carried out for Wave 3, in which 440 emulators were assessed as valid. In Wave 3, the cut-offs of the implausibility measures were changed form previous waves, as the ability of the emulators to refocus with the Wave 1 and 2 cut-offs was diminished. Table \@ref(tab:hm-summary) shows a summary of the number of training runs and the values of the cut-offs used in each wave. The table also shows the proportion of the original input space evaluated as non-implausible after each run. This is computed as the proportion of proposal samples evaluated as non-implausible in the run multiplied by the proportion of the original space deemed non implausible in the previous run. For example, in Wave 2, 18043/35715 = 0.505 of the runs were evaluated as non implausible, multiplied by 0.19 for Wave 1 gives 0.09. Implicitly, the proportion of the input space that is plausible before Wave 1 (wave 0) is 1. From Table \@ref(tab:hm-summary) it can be seen that it was possible to go from a state of complete ignorance about the values of the state parameters to a subset of values 0.03% the size of the original volume for the price of about 1300 TALYS runs. 

\begin{table}\centering
\label{hm-summary}
\caption{Summary of history matching waves. Column two indicates how many simulator runs were used to build the emulators at each wave. Columns three and four are the cutoff values for the univariate and multivariate implausibility measures used in each wave. Column five indicates the proportion of the original design space evaluated as non-implausible after each wave.}
\begin{tabular}{ccccc}\\
Wave & Runs &$I_i^{(2M)}$&$I_i^{(MV)}$& \% of Original Space \\
\hline
1 & 300 & 3 & $\chi^2_{464,0.99}$ & 0.19 \\
2 & 276 & 3 & $\chi^2_{425,0.99}$ &  0.09 \\
3 & 600 & 2 & $\chi^2_{440,0.95}$ & 0.03 \\
\hline
\end{tabular}
\end{table}

In Wave 3 3572 proposal inputs were examined, and 1115 were evaluated as non-implausible, about 31%. This was the final wave, and consequently the 1115 were the final non-implausible inputs. The joint distribution of these samples contained useful information about the relationship between the active parameters and the TALS outputs and could also be used to allow sampling from the conditional distribution of the parameters given the data, as discussed in the next Section. 


## Posterior sampling of active inputs {#posterior-sampling}

Eventually the iterative history matching process must stop. This might be because there is enough evidence to suggest that further waves will not reduce the non-implausible input space much further, or that there is a suitable probability of choosing a non-implausible random sample from the remaining active parameter space, such as in [@jeremy_histmatch]. In this dissertation the process was stopped as all the time/compute resource available was used up. Results that arise in this way can still be useful due to the rapid reduction of the plausible parameter space that occurs in early waves, as can be seen in Table \@ref(hm-summary). 

The endpoint of nuclear data evaluation is a set of multivariate normal distributions, one for each reaction, describing a suite of cross-sections on a fine energy grid. Consequently, the non-implausible active parameter space,  denoted $\{\mathbf{x_{ni}}\}$, is of secondary interest, the primary interest is in the implication for the distribution of non-implausible TALYS outputs. One way to learn about the distribution of non-implausible TALYS outputs would be through Monte Carlo (MC) sampling, where samples are drawn from the posterior distribution of active parameters and TALYS is run for each sample. The resulting TALYS outputs can be used to learn the properties of the required multivariate normal distribution, and should converge in distribution towards it. This requires being able to generate samples from the required posterior. The non-implausible parameter space is not an estimate of the posterior distribution of $\mathbf{x}$, denoted $\pi(\mathbf{x}|z)$, but it does contain it. Consequently it was possible to sample from it, using the following method, taken from [@eremy_histmatch].

A proposal density was required to allow random sampling of proposal posterior samples. A sensible choice was

\begin{equation}
\label{eq:proposal}
P(\mathbf{x}) = N\left(\mathbf{x}|\hat{\mathbf{\mu}}_x, \kappa\hat{\mathbf{\Sigma}}_x \right)
\end{equation}

where $\hat{\mathbf{\mu}}_x$ and $\hat{\mathbf{\Sigma}}_x$ were the sample mean and covariance matrices of $\{\mathbf{x_{ni}}\}$. Random samples were drawn from Equation \@ref(eq:proposal), each one a proposal for a sample from $\pi(\mathbf{x}|z)$. The constant $\kappa$ was used to 'widen' the search area, which has been seen to make the posterior sampling more effective [@jeremy_histmatch]. The first step was to draw a large set of proposal samples from Equation \@ref(eq:proposal). In this dissertation, this was done with the aid of the R package 'mvtnorm' [@mvtnorm]. $\kappa$ was set equal to 2.

The likelihood of the data with respect to the proposal samples needed be computed. As an approximation to the likelihood


\begin{equation}
\label{eq:likelihood-posterior}
L(\mathbf{x}) = p\left(\mathbf{z}| \mathbf{x}\right) = N\left( \mathbf{z}| E[g(\mathbf{x})], V(\mathbf{x})\right)
\end{equation}

was used.

Each time a proposal was generated from $P(\mathbf{x})$, it was used to predict the observations $\mathbf{z}$ using the appropriate GP emulator. The mean vector in Equation \@ref(eq:likelihood) was the vector of predictive emulator means at $\mathbf{x}$ corresponding to the observed $\mathbf{z}$. The covariance matrix was constructed by putting the sum of the emulator predictor variance, the observation uncertainty and simulator inadequacy as the diagonals, and zeros for the off diagonals. The likelihood of the data was then computed using 'mvtnorm'. This is the multivariate equivalent of using the 'dnorm' function from base R, that is the probability density, $L(\mathbf{x})$ with respect to Equation \@ref(eq:likelihood) was computed with respect to $\mathbf{z}$.

The likelihood $p(\mathbf{x})$ was also computed with respect to $\mathbf{x}$, and a weight $w(x) = \frac{L(\mathbf{x})}{p(\mathbf{x})}$ was assigned to $\mathbf{x}$. $L(\mathbf{x})$ is a measure of the probability of observing the data given the proposal $\mathbf{x}$ and $p(\mathbf{x})$ as the probability of having proposed $\mathbf{x}$ in the first place.

Once weights have been computed for all the proposal $\mathbf{x}$, if the proposals are randomly sampled with respect to their weights, this is approximately equivalent to sampling from the posterior $\pi(\mathbf{x}|\mathbf{z})$. The weighted sampling was achieved using the R function 'sample' with the 'prob' option set equal to the vector of weights.

This approach requires assuming that the simulator inadequacy and observation uncertainty terms were normally distributed, which is a common assumption, and is implied in Section \@ref(HM-Uncertainty). Implemented in this way, the sampling method also requires that the prior distribution of $\mathbf{x}$ is constant over the likelihood domain. This assumption is often reasonable giving how small the non-implausible space is with respect to the original parameter space after history matching. 

In this dissertation, implementing this method proved challenging. Evaluating $L(\mathbf{x})$ almost always led to values below machine precision, effectively zero. One possible reason for this is that the history matching stopped at wave 3, and the non implausible space was not yet small enough to allow the emulators to predict very well. Hence the likelihood of the data given the parameters was vanishingly small. It may have also been due to the number of relevant observations, 440 for Wave 3. Given that the probability density must integrate to one it was perhaps not unusual that many instantaneous values for the density function of a 440-dimensional Gaussian would be vanishingly small. Some investigation revealed that reducing the number of relevant data points did indeed result in non-zero results for $L(\mathbf{x})$. Consequently it was decided to generate posterior samples of $\mathbf{x}$ with respect to a subset of the observations $\mathbf{z}$. As the elements of $\mathbf{z}$ each corresponded to a different reaction (see Section \@ref(Background:data) ), a natural way to subset the observations was by reaction. From Section \@ref(GPR:GPR), any subset of a multivariate normal distribution is itself normal, consequently any marginal likelihood of Equation \@ref(eq:likelihood) is also normal, and it was possible follow the procedure to generate samples from the marginal posterior densities. This also provided an interesting way to investigate if different values of the parameters were better at reproducing different reactions. This sampling approach generated samples from the posterior that were non-zero outside of the allowable range of values ([-1,1] on the log scale). This was addressed by implying a uniform prior on [-1,1] and 0 otherwise, which translates in practical terms to zero-weighting any posterior samples generated outside the allowable. range

Plots for the posterior samples generated in this way  are shown in Figures \@ref(np-posterior) and \@ref(na-posterior). The likelihood was evaluated with respect to experimental data for these reactions for which there were valid emulators, that is for three data points for reaction (n,a) and fifty data points for reaction (n,p). The posterior samples are fairly uniform across most of the parameters, which may indicate that the two reaction types are fairly insensitive to the settings of most of the parameters, at least at the observed energy states, or that further waves of history matching may need to be carried out in order to build better emulators and increase the 'signal' in the posterior sampling. However, in both Figures, it is evident that lower values of rvadjust and rwdadjust are favoured, and this is particularly evident in Figure \@ref(na-posterior). Since there is evidence of this in both Figures, there is a good chance that lower values of these parameters are characteristics of the scenario being examined, neutrons incident upon Iron-56, rather than one of the reactions that can occur under this scenario. Contrastingly, the (n,a) reaction appears not to favour any particular values for the v3adjust parameter, whereas the (n,p) reaction appears to favour higher values, which could indicate that the parameter is of particular importance in modelling that reaction. 



```{r np-posterior, fig.cap="Posterior samples for active input parameters when the approximate likelihood is evaluated with respect to (n,p) experimental data. This reaction appears to favour lower values for parameters rvadjust and rwdadjust and higher values for v3adjust."}
knitr::include_graphics("~/Maths/Sheffield/dissertation/BookdownTemplate/Dissertation/figures/np_posterior.pdf")
```

```{r na-posterior, fig.cap="Posterior samples for active input parameters when the approximate likelihood is evaluated with respect to (n,a) experimental data. This reaction shows a preference for lower values of rvadjust and rwdadjust."}
knitr::include_graphics("~/Maths/Sheffield/dissertation/BookdownTemplate/Dissertation/figures/na_posterior.pdf")
```


