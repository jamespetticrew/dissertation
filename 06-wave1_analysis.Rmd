# Results {#results}
```{r}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```



```{r, data import, include=FALSE,echo=FALSE}
library(magrittr)
# Pull out 5 < E < 10 MeV
fe_exp <- tibble::as_tibble(
  readRDS("~/Maths/Sheffield/dissertation/pipeline/01/expDt.rda") ) %>%
  dplyr::select(c("REAC","L1","DATA")) %>%
  dplyr::rename("Energy" = "L1", "Cross-section" = "DATA")%>%
  dplyr::filter(Energy>=5. & Energy<=10.) 
fe_exp$REAC <- dplyr::recode_factor(
  fe_exp$REAC,
  "(26-FE-56(N,A)24-CR-53,,SIG)" = "(n,a)",
  "(26-FE-56(N,INL)26-FE-56,,SIG)" = "(n,n')",
  "(26-FE-56(N,P)25-MN-56,,SIG)" = "(n,p)",
  "(26-FE-56(N,TOT),,SIG)" = "(n,tot)",
  "(26-FE-56(N,EL)26-FE-56,,SIG)" = "(n,n)")

lhs_samples <- readr::read_csv("~/Maths/Sheffield/dissertation/exp_design/lhs_samples.csv")
run_data <- readr::read_csv("~/Maths/Sheffield/dissertation/data/train_data_1_logscale.csv")%>%
  tidyr::drop_na()

mahals <- readr::read_rds("~/Maths/Sheffield/dissertation/models/wave1/R/rData/mahal_distances.rds")
chi_df<- readr::read_rds("~/Maths/Sheffield/dissertation/models/wave1/R/rData/chi_sq_df.rds")
plausibles <- readr::read_rds("~/Maths/Sheffield/dissertation/models/wave1/R/rData/plausibles.rds")

na <- readr::read_rds("~/Maths/Sheffield/dissertation/BookdownTemplate/Dissertation/data/na_posterior.rds")
np <- readr::read_rds("~/Maths/Sheffield/dissertation/BookdownTemplate/Dissertation/data/np_posterior.rds")
```

## Introduction

In this section is described the history matching that was carried out using the simulator TALYS and experimental data from EXFOR as described in Chapter \@ref(Background), following the methodology in described in Chapter \@ref(History-Matching), and emulating the simulator as described in Chapter \@ref(GPR). Three waves of history matching were carried out. The first wave and subsequent analysis is described in detail in Sections \@ref(wave1-methodology), \@ref(wave1-analysis) and \@ref(impl-optical-depth). The results and analysis from subsequent waves are given in Sections \@ref(subsequent-waves) and \@ref(posterior-sampling).

## First wave methodology {#wave1-methodology}

A total of 28 active input parameters were considered in the history matching analysis. The design grid for the first wave training runs were generated as follows. A 300 point Latin hypercube design was generated on U$[0,1]^{28}$ using the function 'maximinLHS' from the R package 'lhs' [@lhs_r]. These points then underwent a linear transformation onto U$[-1,1]^{28}$ and finally were exponentiated with base 10. This enabled exploration of the active parameter space both above and below the default parameter values (see Section \@ref(Background:talys)). 

The simulator runs were carried out in parallel across 15 nodes on a personal laptop. The 300 simulator evaluations took approximately 36 hours to complete. The simulator produced cross-section spectra across the four reactions for which relevant experimental data were available, as shown in Table \@ref(tab:run-data), where it can be seen that six of the runs failed to complete successfully for the reactions (n,a), (n,n') and (n,p) (see Section \@ref(Background:data) for an explanation of this notation). 

```{r run-data, fig.cap="Counts of succesful simulator runs for the four reactions for which relevant experimental data exist. Four of the runs failed for partial cross-sections ((n,a), (n,n'), (n,p) ) but still produced total cross-section results."}
run_data %>% tidyr::drop_na() %>% dplyr::group_by(Reaction) %>% 
  dplyr::summarise(count = dplyr::n()) %>%
  dplyr::filter(Reaction %in% c("(n,tot)", "(n,n')", "(n,p)", "(n,a)" )) %>%
  knitr::kable(format="latex",caption="Counts of cross-section spectra generated by TALYS across 300 runs. Not all simulator runs were succesful, but this does not prevent history matching from being carried out." ) 
```

Individual univariate Gaussian process emulators were built for each of the 588 TALYS outputs $f$ corresponding to the 588 experimental observations. A first order linear mean function was used for each emulator, which required learning 29 parameters from the data. Constant and 0 mean functions were also examined, but these led to an unacceptably large number of model failures arising from singular posterior correlation matrices. A Matern correlation function with smoothness parameter $\frac{5}{2}$ was used to aid the parameter estimation routine (Section \@ref(model-selection)). The models were trained allowing a residual nugget to be estimated. This required learning a further 30 hyperparameters; 28 length-scale parameters, one variance and one residual nugget. None of the models failed to build. The inputs were transformed back to [-1,1] for training the models. The outputs were left unscaled in order to avoid potential errors when re-scaling the error terms for computing the implausibility measures.

A further set of 30 test runs were carried out, with the design matrix being generated using the same principles used to generate the training designs. The emulators were validated by predicting at the values of the inputs for the training runs and then computing the scaled Mahalanobis distances between the predicted outputs and the simulator outputs. If the assumptions of the Gaussian process emulator are valid, these distances are hypothesised to have a scaled-$F_{m,n-q}$ distribution [@gp_diagnostics], where $m$ is the number of test points (30), $n$ is the number of training points, which varies across models, and $q$ is the number of parameters in the emulator mean function (29). The distance metrics were compared to critical values corresponding to the one and 99 percentiles of the appropriate F distributions and the models were discarded if they lay outside this 98% probability interval. As a result of this, 2% of the valid emulators could be discarded on average, which is the price of protecting against the consequences of predicting with bad emulators. Of the 588 models, 124 were discarded this way, leaving 464.

Figure \@ref(fig:dodgy-emulator) illustrates the importance of carrying out emulator evaluation. The Figure shows evaluation run results for an emulator for the simulated cross-section at 6.19 MeV for the reaction (n,p). This emulator was rejected at evaluation. The vertical lines show the two standard deviation predictive intervals for the emulator and the line x=y is plotted for reference. Also shown, at the intersection of the horizontal and vertical lines is the nominal measured cross-section value at 15.2 barns. The job of the emulator is to try and predict 15.2 barns at the same active parameter values as the simulator predicts 15.2 barns. From the plot, it appears that the emulator consistently under-predicts the simulator in this region. Consequently, the emulator would predict that the cross-section would be close to 15.2 barns in a region of parameter space where the simulated cross-section would be considerably higher. As a consequence of this, a non-implausible value of the active parameters could be rejected as implausible, and the opportunity to run the simulator at this input and better emulate close to the observed value could be lost. Hence, it is important to carry out validation on the emulators to ensure good out-of-sample predictive performance. 

```{r dodgy-emulator, fig.cap="Validation run results for an emulator for the cross-section for the (n,p) reaction at energy 6.19 MeV. The vertical lines show the two standard deviation predictive intervals of the emulator and the x=y line has been plotted for reference. The observed cross section value is shown as the intersection of the vertical and horizontal line at (15.2,15.2). It appears that the emulator would underpredict the simulator output at this point. "}
knitr::include_graphics("~/Maths/Sheffield/dissertation/BookdownTemplate/Dissertation/figures/dodgynp6-19.pdf")
```


## First wave analysis {#wave1-analysis}

The computed scaled Mahalanobis distances for the accepted models for (n,tot) are plotted in Figure \@ref(fig:mahalanobis). The hypothesised scaled $F_{30,271}$ curve is plotted over the top. The distribution of distances should resemble this curve under Gaussian process emulator assumptions. The histogram and the curve appear to have reasonably similar densities, although the mode is shifted slightly right. 

```{r mahalanobis, fig.cap= "Histogram of scaled Mahalanaobis distances for valid (n,tot) models. The hypothesised scaled-$F_{30,271}$ curve is plotted over the top. The distribution of Mahalanobis distances looks reasonably similar to the hypothesised distribution."}
mahals_sub <- mahals %>%
  dplyr::filter(reaction!="(n,g)")

x <- seq(0,3,length.out = 100)
m <- 30
n <- 300
q <- 29
y <- df(x, df1 = m, df2 = (n-q))
valid_models <- mahals_sub %>%
  dplyr::filter(prob > .01 & prob < .99) 
valid_models %>%  dplyr::filter(reaction == "(n,tot)") %>%
  dplyr::pull(scaled) %>% hist(freq=F,ylim=c(0,3),xlim=c(0,3), main=NULL) 
lines(x,y)
```

The 464 models were used to find non-implausible active parameter values for wave two. The number of potential points that could be evaluated was limited by memory and compute power. A total of 142860 points were selected at random from the 28 dimensional active parameter space were selected. Implausibility metrics were generated for each of the 464 models for each of the 142860 proposal points. Equations \@ref(eq:one-d-implausibility) was evaluated to compute 464 one-dimensional implausibility measures for each proposal $\mathbf{x}$ and Equation \@ref(eq:chi-sq-impl) was evaluated to compute one multidimensional implausibility metric for each proposal $\mathbf{x}$ using the corresponding experimental observations and uncertainties as described in Chapter \@ref(History-Matching).

Two criteria were used for accepting a proposal point $\mathbf{x_j}$, First that $I_j^{(uv)}<3$ and second that $I^{(mv)}_j < \chi^2_{464,.95}$. The second highest of the 464 implausibility measures for proposal $\mathbf{x_j}$was chosen as $I_j^{(uv)}$. 272 points met these criteria - 0.019% of the proposed points. 

 Different acceptance criteria for the two implausibility measures were examined; Table \@ref(tab:cutoffs) shows some of the results of this analysis, where it can be seen that the choice which univariate implausibility measure to use was the most important decision in determining the size of the non-implausible active parameter space. Interestingly, choosing the highest univariate implausibility for each proposal resulted in zero non-implausible samples, and so no one sample was non-implausible for every process.

\begin{table}\centering
\caption{Number of proposal points accepted as a function of different cut-offs for implausibility metrics. $I^{uv=a}$ indicates the $a$th largest univariate implausibility measure for that proposal point - the number of chosen points is more sensitive to the choice for this metric than to the choice of chi-squared cut-off percentile.}
\label{tab:cutoffs}
\begin{tabular}{cccc}\\
&$I_i^{(uv=1)}$&$I_i^{(uv=2)}$&$I_i^{(uv=3)}$\\
\hline
$\chi^2_{464,.95}$& 0 & 268 & 4381 \\
$\chi^2_{464,.99}$& 0 & 276  & 4472 \\
\hline
\end{tabular}
\end{table}

## Minimum implausibility and optical depth analysis {#impl-optical-depth}

The sensitivity of implausibility to the inputs can be visualised in plots such as those in Figure \@ref(fig:implausibility-plots). The left column shows minimum implausibilities and the right column shows plots of probabilities for two of the inputs. The minimum implausibility plots were generated by subdividing the $[-1,1]^2$ grid (the log$_{10}$ of the parameter space) for two of the input parameters into 100 blocks. Each proposal input was then put into one of these intervals according to its values of the two relevant inputs. The minimum univariate implausibility measure over all the candidate $\mathbf{x}$s from each interval was then pulled out and used to determine the hue in that interval in the heat map. The code used to generate these plots is shown in Appendix \@ref(r-code-impl). 

If the value of the minimum implausibility was high, this might indicate that it is not possible to get TALYS to match experimental measurements when run at that combination of values for those elements of $\mathbf{x_j}$. The optical depth plots were constructed by grouping the proposal inputs in the same way. An estimate of the probability of finding a non-implausible input in an interval was then computed as the ratio of the number of non-implausible inputs to the total number of proposal inputs in that interval. Taken together, the plots help in visualising the sensitivity of TALYS to different values of the inputs. 


```{r implausibility-plots,fig.cap="Minimum implausibility plots (left column) and optical depth plots (right column) for six of the inputs. The minimum implausibilty plots show estimates of the minimum univariate implausibility for values of two of the inputs and the optical depth plots show estimates of the probability of finding a non-implausible input for values of two of the inputs. Note that the heatmaps are not all on the same scale.",out.width="100%",out.height="200%"}
knitr::include_graphics("~/Maths/Sheffield/dissertation/BookdownTemplate/Dissertation/figures/impl_plot2.png")
```


The first row of Figure \@ref(fig:implausibility-plots) shows implausibility and optical depth plots for parameters 'd1adjust' and 'rvadjust'. These plots indicate that larger positive values for 'rvadjust' are more likely to cause TALYS to provide a poor match to observations, and that values below zero (that is multipliers below one, so values for 'rvadjust' less than the default) are more likely to produce non-implausible results. The optical depth plot indicates that there might be some interaction effect between 'd1adjust' and 'rvadjust', as indicated by the higher probability region in the top left corner of the plot. The second row of Figure \@ref(fig:implausibility-plots) shows plots for parameters 'awadjust' and 'rvsoadjust'. This plot also suggets that there may be an interaction between the two parameters, inputs with values of 'awadjust' close to -1 and 'rvsoadjust' close to +1 being more likely to produce non-implausible outputs. The bottom row shows plots for the parameters 'rwadjust' and 'd3adjust' and suggests that values of 'rwadjust' close to the TALYS default are more likely to give non-implausible outputs, and that perhaps the outputs are not very sensitive to the value for 'd3adjust'. 

## Subsequent waves {#subsequent-waves}

The 276 non-implausible inputs chosen in Section \@ref(wave1-analysis) were used as the design for the training runs in wave two. A new set of 588 emulators were built. A further set of test runs of the simulator were carried out. The active input parameter values for these test runs were chosen by generating a large number of proposal samples and evaluating their implausibility using the wave one emulators. The wave two emulators were then validated in the same way as described in Section \@ref(wave1-methodology). 163 were invalidated, leaving 425 emulator-measurement pairs with which to carry out implausibility analysis for wave 2. Less computational resource was available at the time of the wave two implausibility analysis, and consequently only 35715 proposal points were examined. Of these, 18043 were accepted as non-implausible, which is just over 50%. The cut-offs used for the implausibility measures were the same as those used in Section \@ref(wave1-analysis), although the relevant $\chi^2$ distribution from which the 99th percentile was taken was different, due to their being a different number of validated model between the waves (see Section \@ref(HM-Multi-D)).

Having such a large number of non-implausible points for wave two presented a new challenge, as it was impossible to run TALYS for all 18043 non-implausible inputs in an acceptable time interval. Consequently a subset of the inputs had to be selected. It was decided to run the simulator at 600 training points for wave three (plus 60 test points). Instead of randomly sampling 600 of the 18043 non-implausible proposals, an attempt was made to find an optimal set of points by maximising the minimum distance between 28 input coordinates. This was carried out by first choosing a random sample of 600 points and computing their distance matrix (using the R function 'dist'). The minimum value in the matrix was extracted. This process was repeated for $10^5$ random samples of 600. The sample that had the maximum minimum distance was used as the design for wave three, which was a way of attempting to select a set of design points that were as spread out across the non-implausible volume as possible. 

The same workflow for building and validating the emulators was carried out for Wave 3, in which 440 emulators were assessed as valid. In wave three, different cut-offs were used for the implausibility measures, as the ability of the emulators to refocus with the wave one and two cut-offs was diminished. Table \@ref(tab:hm-summary) shows a summary of the number of training runs and the values of the cut-offs used in each wave. The table also shows the proportion of the original input space evaluated as non-implausible after each run. This is computed as the proportion of proposal samples evaluated as non-implausible in the run multiplied by the proportion of the original space deemed non implausible in the previous run. For example, in wave teo, 18043/35715 = 0.505 of the runs were evaluated as non implausible. The proportion of non-implausible space from wave one was 0.19. The product of these proportions is 0.09. Implicitly, the proportion of the input space that is plausible before wave one (wave zero) is 1. From Table \@ref(tab:hm-summary) it can be seen that it was possible to go from a state of complete ignorance about the values of the state parameters to a subset of values 0.03% the size of the original volume for the price of about 1300 TALYS runs. 

\begin{table}\centering
\label{tab:hm-summary}
\caption{Summary of history matching waves. Column two indicates how many simulator runs were used to build the emulators at each wave. Columns three and four are the cutoff values for the univariate and multivariate implausibility measures used in each wave. Column five indicates the proportion of the original active parameter space evaluated as non-implausible after each wave.}
\begin{tabular}{ccccc}\\
Wave & Runs &$I_j^{(uv)}$&$I_j^{(mv)}$& \% of Original Space \\
\hline
1 & 300 & 3 & $\chi^2_{464,0.99}$ & 0.19 \\
2 & 276 & 3 & $\chi^2_{425,0.99}$ &  0.09 \\
3 & 600 & 2 & $\chi^2_{440,0.95}$ & 0.03 \\
\hline
\end{tabular}
\end{table}

In the third wave, 3572 proposal inputs were examined, and 1115 were evaluated as non-implausible, about 31%. This was the final wave, and consequently the 1115 were the final non-implausible inputs. These samples contained useful information about the relationship between the active parameters and the TALYS outputs and could also be used to allow sampling from the posterior distribution of the active parameters (conditional on the data), as discussed in the next Section. 


## Posterior sampling of active inputs {#posterior-sampling}

Eventually the iterative history matching process must stop. This might be because there is enough evidence to suggest that further waves will not reduce the non-implausible input space much further, or that there is a suitable probability of choosing a non-implausible random sample from the remaining active parameter space, such as in [@jeremy_histmatch]. In this dissertation the process was stopped as all the time/compute resource available was used up. Results that arise in this way can still be useful due to the rapid reduction of the plausible parameter space that occurs in early waves, as can be seen in Table \@ref(tab:hm-summary). 

One endpoint of nuclear data evaluation is a set of multivariate normal distributions, one for each reaction, describing a suite of cross-sections on a fine energy grid. Consequently, the non-implausible active parameter space, denoted $\{\mathbf{x_{ni}}\}$, is of secondary interest and the primary interest lies in the implication for the distribution of non-implausible TALYS outputs. One way to learn about the distribution of these non-implausible outputs would be through Monte Carlo (MC) sampling, where TALYS is run at samples of $\mathbf{x}$ drawn from the posterior distribution of active parameters. The results can be used to learn the properties of the required multivariate normal distributions. This requires being able to generate samples from the required posterior. The non-implausible parameter space is not an estimate of the posterior distribution of $\mathbf{x}$, denoted $\pi(\mathbf{x}|z)$, but it does contain it. Consequently it was possible to sample from it, using the following method, taken from [@jeremy_histmatch].

A proposal density was required to allow proposal posterior samples to be generated. A sensible choice was

\begin{equation}
\label{eq:proposal}
P(\mathbf{x}) = N\left(\mathbf{x}|\hat{\mathbf{\mu}}_x, \kappa\hat{\mathbf{\Sigma}}_x \right)
\end{equation}

where $\hat{\mathbf{\mu}}_x$ and $\hat{\mathbf{\Sigma}}_x$ were the sample mean and covariance matrices of $\{\mathbf{x_{ni}}\}$. Random samples were drawn from Equation \@ref(eq:proposal), each one a proposal for a sample from $\pi(\mathbf{x}|z)$. The constant $\kappa$ was used to 'widen' the search area, which has been seen to make the posterior sampling more effective [@jeremy_histmatch]. The first step was to draw a large set of proposal samples from Equation \@ref(eq:proposal). In this dissertation, this was done with the aid of the R package 'mvtnorm' [@mvtnorm]. $\kappa$ was set equal to 2.

The likelihood of the data $\mathbf{z}$ with respect to the proposal samples needed be computed for each proposal. As an approximation to the likelihood


\begin{equation}
\label{eq:likelihood-posterior}
L(\mathbf{x}) = p\left(\mathbf{z}| \mathbf{x}\right) = N\left( \mathbf{z}| E[g(\mathbf{x})], V(\mathbf{x})\right)
\end{equation}

was used.

Each time a proposal $\mathbf{x}$ was generated from $P(\mathbf{x})$, the wave three emulator predictions at this point $\mathbf{\hat{f}(x)}$ were computed. The mean vector in Equation \@ref(eq:likelihood-posterior) was the vector of predictive emulator means at $\mathbf{x}$, each element a prediction of an observation from $\mathbf{z}$. The covariance matrix was constructed by putting the sum of the emulator predictor variance, the observation uncertainty and simulator inadequacy as the diagonals, and zeros for the off diagonals. The likelihood of the data was then computed using 'mvtnorm'. This is the multivariate equivalent of using the 'dnorm' function from base R, that is the probability density, $L(\mathbf{x})$ with respect to Equation \@ref(eq:likelihood-posterior) was computed with respect to $\mathbf{z}$.

The likelihood $P(\mathbf{x})$ was also computed with respect to $\mathbf{x}$, and a weight $w(x) = \frac{L(\mathbf{x})}{P(\mathbf{x})}$ was assigned to $\mathbf{x}$. $L(\mathbf{x})$ is a measure of the probability of observing the data given the proposal $\mathbf{x}$ and $P(\mathbf{x})$ as the probability of having proposed $\mathbf{x}$ in the first place.

Once weights have been computed for all the proposal $\mathbf{x}$, if the proposals are randomly sampled with respect to their weights, this is approximately equivalent to sampling from the posterior $\pi(\mathbf{x}|\mathbf{z})$. The weighted sampling was achieved using the R function 'sample' with the 'prob' option set equal to the vector of weights.

This approach requires that the simulator inadequacy and observation uncertainty terms were normally distributed, an assumption that is commonly leveraged [@bower2010galaxy]. Implemented in this way, the sampling method also requires that the prior distribution of $\mathbf{x}$ is constant over the likelihood domain. This assumption is often reasonable giving how small the non-implausible space is with respect to the original parameter space after history matching. 

In this dissertation, implementing this method proved challenging. Evaluating $L(\mathbf{x})$ almost always led to values below machine precision, effectively zero. One possible reason for this is that the history matching stopped at wave three, and the non-implausible space was not yet small enough to allow the emulators to predict very well. Hence the likelihood of the data given the parameters was vanishingly small. It may have also been due to the number of relevant observations, 440 for wave three. Given that the probability density must integrate to one it was perhaps not unusual that many instantaneous values for the density function of a 440-dimensional Gaussian would be vanishingly small. Some investigation revealed that reducing the number of relevant data points did indeed result in non-zero results for $L(\mathbf{x})$. Consequently it was decided to generate posterior samples of $\mathbf{x}$ with respect to a subset of the observations $\mathbf{z}$. As the elements of $\mathbf{z}$ each corresponded to a different reaction (see Section \@ref(Background:data)), a natural way to subset the observations was by reaction. From Section \@ref(GPR:GPR), any subset of a multivariate normal distribution is itself normal, consequently any marginal likelihood of Equation \@ref(eq:likelihood) is also normal, and it was possible follow the procedure to generate samples from the marginal posterior densities of the active parameters. This also provided an interesting way to investigate if different values of the parameters were better at reproducing different reactions. 

This approach resulted in samples from the posterior being generated that were non-zero outside of the allowable range of values ([-1,1] on the log scale). This was addressed by implying a uniform prior on [-1,1] and 0 otherwise, which translates in practical terms to zero-weighting any posterior samples generated outside of the interval [-1,1].

Plots for the posterior samples generated in this way  are shown in Figures \@ref(fig:np-posterior) and \@ref(fig:na-posterior). The likelihood was evaluated with respect to the experimental data for these reactions for which there were valid emulators, that is for three data points for reaction (n,a) and fifty data points for reaction (n,p). 

The posterior samples are fairly uniform across most of the parameters, which may indicate that the two reaction types are fairly insensitive to the settings of most of the parameters, at least at the observed energy states, or that further waves of history matching may be needec in order to build better emulators and increase the 'signal' in the posterior sampling. However, in both figures, it is evident that lower values of rvadjust and rwdadjust are favoured, and this is particularly evident in Figure \@ref(fig:na-posterior). Since there is evidence of this in both figures, there is a good chance that lower values of these parameters are characteristics of the scenario being examined, neutrons incident upon Iron-56, rather than characteristic of a particular reaction. Contrastingly, the (n,a) reaction appears not to favour any particular values for the v3adjust parameter, whereas the (n,p) reaction appears to favour higher values, which could indicate that the parameter is of particular importance in modelling that reaction. 
```{r np-posterior, fig.cap="Posterior samples for active input parameters when the approximate likelihood is evaluated with respect to (n,p) experimental data. This reaction appears to favour lower values for parameters 'rvadjust' and 'rwdadjust' and higher values for 'v3adjust'."}
knitr::include_graphics("~/Maths/Sheffield/dissertation/BookdownTemplate/Dissertation/figures/np_posterior.pdf")
```

```{r na-posterior, fig.cap="Posterior samples for active input parameters when the approximate likelihood is evaluated with respect to (n,a) experimental data. This reaction shows a preference for lower values of 'rvadjust' and 'rwdadjust'."}
knitr::include_graphics("~/Maths/Sheffield/dissertation/BookdownTemplate/Dissertation/figures/na_posterior.pdf")
```

## Conclusion

In this chapter, three waves of history matching were carried out, which was seen to shrink the non-implausible active parameter space to 0.03% of its original size. Sensitivity of the simulator output to a subset of the active parameters was analysed. Samples were drawn from the marginal posterior distributions of the active parameters.

The next chapter ends the dissertation with a discussion on the analysis and on how it could be improved upon and extended. 

