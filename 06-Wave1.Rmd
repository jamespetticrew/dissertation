---
output:
  pdf_document: default
  html_document: default
---

```{r}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```



```{r data import, include=FALSE,echo=FALSE}
library(magrittr)
# Pull out 5 < E < 10 MeV
fe_exp <- tibble::as_tibble(
  readRDS("~/Maths/Sheffield/dissertation/pipeline/01/expDt.rda") ) %>%
  dplyr::select(c("REAC","L1","DATA")) %>%
  dplyr::rename("Energy" = "L1", "Cross-section" = "DATA")%>%
  dplyr::filter(Energy>=5. & Energy<=10.) 
fe_exp$REAC <- dplyr::recode_factor(
  fe_exp$REAC,
  "(26-FE-56(N,A)24-CR-53,,SIG)" = "(n,a)",
  "(26-FE-56(N,INL)26-FE-56,,SIG)" = "(n,n')",
  "(26-FE-56(N,P)25-MN-56,,SIG)" = "(n,p)",
  "(26-FE-56(N,TOT),,SIG)" = "(n,tot)",
  "(26-FE-56(N,EL)26-FE-56,,SIG)" = "(n,n)")

lhs_samples <- readr::read_csv("~/Maths/Sheffield/dissertation/exp_design/lhs_samples.csv")
run_data <- readr::read_csv("~/Maths/Sheffield/dissertation/data/train_data_1_logscale.csv")%>%
  tidyr::drop_na()

mahals <- readr::read_rds("~/Maths/Sheffield/dissertation/models/wave1/R/rData/mahal_distances.rds")
chi_df<- readr::read_rds("~/Maths/Sheffield/dissertation/models/wave1/R/rData/chi_sq_df.rds")
plausibles <- readr::read_rds("~/Maths/Sheffield/dissertation/models/wave1/R/rData/plausibles.rds")
```

# History matching first wave {#wave1}

The case study focussed on neutron cross-sections for the Fe-56 isotope for incident neutron energies between 5 and 10 MeV. Experimental data for incident neutrons in this energy interval was available for five key reactions as shown in Table 1. The data are dominated by total cross-section measurements. One run of the simulator can generate energy-dependent cross-sections across multiple reactions on a user-defined energy grid. The computational cost rises with the size of the energy grid. Consequently, it was decided to only run the simulator at half of the energies for which total cross-section measurements were available. The simulator was run at every energy for which other measurements were available. After accounting for duplicated energies the simulator was run on a 593-point energy grid.

A total of 28 simulator inputs were varied. All inputs were multipliers for parameters in the nuclear models used by TALYS to compute the cross-sections. Each parameter has a default value, and if its 'adjust' multiplier is not specified in the input file, then the multiplier is implicitly one. The range for all multipliers was [0,10]. It was assumed that the 'best' value for each parameter was just as likely to be below its default as above it. Consequently the design grid for the first wave training runs were generated as follows. A 300 point maxmin Latin hypercube design was generated on $U[0,1]^{28}$. These points then underwent a linear transformation onto $U[-1,1]^{28}$ and finally were exponentiated with base ten. This enables exploration of the design space both above and below the default parameter values. Figure 1 shows the distributions of the design points before and after the exponential transformation.

```{r, distribution of inputs, fig.cap="Distributions of design points on exponentiated and linear scales. Exponentiating the multipliers ensures that the region of design space below the default values is explored as well as the region above the defaults."}
lhs_samples %>% tidyr::pivot_longer(
  cols=dplyr::everything(), 
  names_to = "V", values_to = "Exponentiated") %>%
  dplyr::mutate("Linear" = log(Exponentiated, base=10)) %>%
  tidyr::pivot_longer(c("Linear","Exponentiated"), 
                      names_to = "Scale",values_to = "Value") %>%
  ggplot2::ggplot() + ggplot2::geom_histogram(ggplot2::aes(x=Value,fill=Scale) ) +
  ggplot2::facet_wrap(~Scale,ncol=2) + ggplot2::theme_minimal() +
  ggplot2::theme(legend.position = "None", 
                 axis.text.y = ggplot2::element_blank(),
                 axis.title.y = ggplot2::element_blank() )

```

The simulator runs were carried out across 15 nodes on a personal laptop. TALYS is inherently serial, but the design runs were independent, and could be run concurrently. The 300 simulator evaluations took approximately 36 hours to complete. The simulator produced cross-section spectra across 6 reactions, as shown in Table 2, where it can be seen that many of the runs only produced a partial output.

```{r}
run_data %>% tidyr::drop_na() %>% dplyr::group_by(Reaction) %>% 
  dplyr::summarise(count = dplyr::n()) %>%
  knitr::kable(format="latex",caption="Counts of cross-section spectra generated by TALYS across 300 runs. Not all simulator runs were succesful, but this does not prevent history matching from being carried out." ) 
```

Individual univariate Gaussian process emulators were built for each of the 593 energy points for
each of the 4 reactions in common between the observed and simulated cross-section data sets, (n,tot), (n,n'), (n,p) and (n,a), 2372 models in all. This approach was inefficient as there did not exist observations at each of the 593 energy points across all four reactions, and much compute was wasted training and evaluating models for which implausibility measures could not be computed. This was addressed in subsequent waves. The emulators used a first order linear mean function, which required learning 29 parameters from the data. Constant and 0 mean functions were also examined, but these led to an unacceptably large number of model failures arising from singular posterior correlation matrices. A Matern correlation function with smoothness parameter $\frac{5}{2}$ was used. The models were trained allowing a residual nugget to be estimated. None of the models failed to build. The inputs were transformed back to [-1,1]. The outputs were left unscaled in order to avoid potential errors when scaling the error terms for the implausibility measures.

A further set of 30 test runs were carried out, with the design matrix being generated using the same principles used to generate the training designs. The emulators were validated by predicting at the values of the inputs for the training runs and then computing the Mahalanobis distances between the predicted outputs and the simulator outputs. If the assumptions of the Gaussian process emulator are valid, these distances are hypothesised to have a scaled-$F_{m,n-q}$ distribution, where $m$ is the number of test points (30), $n$ is the number of training points, which varies across models, and $q$ is the number of parameters in the emulator mean function (29). The distance metrics were compared to critical values corresponding to the one and 99 percentiles of the appropriate F distributions and the models were discarded if they lay outside this 98% probability interval. Of the 2372 models, 739 were discarded this way, leaving 1633.

The computed Mahalanobis distances for the accepted models for (n,tot) are plotted in Figure 2. The hypothesised scaled $F_{30,271}$ curve is plotted over the top. The distribution of distances should resemble this curve under Gaussian process emulator assumptions. The histogram and the curve appear to have reasonably similar densities, although the mode is shifted slightly right. This analysis was carried out conscious of the pitfalls of carrying out multiple hypothesis tests and the associated compounding of p-values - the test procedure was not intended to be rigorous, but a conservative filter for potentially poor models in a scenario where model scarcity was not an issue. 

```{r, fig.cap= "Histogram of Mahalanaobis distances for valid (n,tot) models. The hypothesised scaled-$F_{30,271}$ curve is plotted over the top. The distribution of Mahalanobis distances looks reasonably similar to the hypothesised distribution."}
mahals_sub <- mahals %>%
  dplyr::filter(reaction!="(n,g)")

valid_models <- mahals_sub %>%
  dplyr::filter(prob > .01 & prob < .99) 
mahals_sub %>%  dplyr::filter(reaction == "(n,tot)") %>%
  dplyr::pull(scaled) %>% hist(freq=F,
                                 ylim=c(0,3), main=NULL) 
x <- seq(0,3,length.out = 100)
m <- 30
n <- 300
q <- 29

y <- df(x, df1 = m, df2 = (n-q))
lines(x,y)
```

Of the 1633 validated models, 464 had corresponding observations allowing for implausibility analysis. Further waves trained models only at energy/ reaction combinations for which observations existed. The 464 models were used to find non--implausible design points for Wave 2. The number of potential design points that could be evaluated was limited by memory and compute power. A total of 142860 random points in the 28 dimensional design space were selected. Implausibility metrics were generated for each of the 464 models for each of the 142860 proposal points. In the implausibility analysis the assumption was made that observation uncertainty, $\sqrt{V_j^{(obs)}}$ for observation $z_j$ was $0.1z_j$ and that the simulator inadequacy, $\sqrt{V^{(s)}_j}$  for simulator output $f_j(\mathbf{x_i})$ was also $0.1z_j$, where $j \in 1,2,...,464$ indexes over the observations/ emulator pairs and $i \in 1,2,...,142860$ indexes over the proposal points, implying that the simulator inadequacy is independent of the design point at which the simulator is evaluated. The third type of uncertainty considered was emulator uncertainty, $\sqrt{V[f_j(\mathbf{x_i})]}$ corresponding to mean emulator output $E[(f_j(\mathbf{x_i})]$. Hence the implausibility measure for proposal point $i$ computed using simulator $j$ is 

\begin{equation}
\label{eq:implausibility}
I_{i,j} = \frac{ | z_j - E[f_j(\mathbf{x_i})]  |   }{ \sqrt{(V_j^{(obs)} + V^{(s)}_j + V[f_i(\mathbf{x_j})]   )   }}.
\end{equation}

Equation (1) was evaluated 464 times for each proposal point. The second largest of the 464 measures $I_i^{(2M)}$ for a proposal point $\mathbf{x_i}$ was used as the first implausibility metric for the proposal point. Another multivariate implausibility was also considered:

\begin{equation}
\label{eq:implausibility2}
I_{i} = \left( \mathbf{z} - E[f(\mathbf{x_i})] \right)^T \left(V_j^{(obs)} + V^{(s)}_j + V[f_i(\mathbf{x_j})] \right)^{-1} \left( \mathbf{z} - E[f(\mathbf{x_i})] \right)
\end{equation}

where $\mathbf{z}$ and $E[f(\mathbf{x_i})]$ are j-vectors with the 464 observations and their corresponding emulator predictions respectively, and $V_j^{(obs)}$, $V^{(s)}_j$ and $V[f_i(\mathbf{x_j})]$ are now all covariances matrices. The simple approach was taken in assuming that the outputs are all uncorrelated. In this case, all three covariance matrices are diagonal, with the square of the denominator in Equation (1) making the $j,j$-th element of the matrix. Taking this approach, $I_i$ for proposal point $i$ is simply $\sum_{j=1}^{464} I^2_{i,j}$.

Two criteria were used for accepting a proposal point $\mathbf{x_i}$, First that $I_i^{(2M)}<3$ and second that $I_i < \chi^2_{464,.95}$. 272 points met these criteria - 0.019% of the proposed points. The decision on how many points to examine, and what proportion of these to accept, was driven chiefly by practical considerations. The lists of implausibility measures for all the proposal points took up 5.4GB in memory, bringing an 8GB laptop close to capacity, and based on the 36 hour run time of the 300 wave one runs, a similar number of accepted points was desired to allow the analysis to be carried out in a reasonable amount of time. Given the size of the input space it would be desirable to generate a much larger proposal sample if resources allowed. Different acceptance criteria for the two implausibility measures were examined; Table 3 shows some of the results of this analysis, where it can be seen that the choice of maximal implausibility was the most important decision in determining the size of the non-implausible design space. 

\begin{table}\centering
\caption{Number of proposal points accepted as a function of different cutoffs for implausibility metrics. $I^{nM}$ indicates the $n$th largest univariate implausibility measure for that proposal point - the number of chosen points is more sensitive to the choice for this metric than to the choice of chi-squared cut-off percentile.}
\begin{tabular}{cccc}\\
&$I_i^{(1M)}$&$I_i^{(2M)}$&$I_i^{(3M)}$\\
\hline
$\chi^2_{464,.95}$& 0 & 268 & 4381 \\
$\chi^2_{464,.99}$& 0 & 276  & 4472 \\
\hline
\end{tabular}
\end{table}

Some sense of the sensitivity of implausibility to the inputs can be gained from examining plots such as those in Figure 3. This plot was generated by grouping the proposal inputs according to the values of two of the inputs ("rwadjust" and "rwdadjust" in Figure 3) and finding the maximal chi-squared implausibility measure for that combination of inputs. The tiles in the plot are coloured according to the log of the value of that chi-squared implausibility (logged because the values differed over several orders of magnitude). Care should be taken in interpreting these plots, as the first Wave emulators are expected to be a poor representation of the simulator, but it appears that higher implausibility is associated with higher values of both variables. Interestingly, this is reflected in the Figure 4 in the "rwdadjust" coordinates of the 276 implausible points, but the "rwadjust" plausible point coordinates appear to favour the region of input space [-0.5,0.5].


```{r heatmaps, fig.cap=""}
knitr
```



