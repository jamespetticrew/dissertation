# Gaussian process regression {#GPR}
 ```{r prelims, include=FALSE}
 knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
 library(magrittr)
library(ggplot2)
 ```
## Introduction

In this section some of the key ideas in Gaussian process regression (GPR) are discussed. Section \@ref(GPR:GPR) gives a brief introductions to GPR and Section \@ref(model-selection) discusses how parameters are estimated. This is followed by content on two key components of GPR: the covariance kernel and mean function. The chapter finishes with a discussion on the software packages with which some of the modelling in this dissertation was attempted, highlighting some of the key features that made them suitable or otherwise for the work.


## Gaussian process regression {#GPR:GPR}

In this section a single output of TALYS $f_i$ is reasoned about. No loss of generality is incurred by proceeding in this way, as all of the TALYS outputs were modelled independently, and the same reasoning applies to all of them equally, so  the $i$ subscript is dropped for convenience.

A GPR model was used as an emulator for TALYS in order to predict, with associated uncertainty estimates, the simulator output $f$ at unobserved inputs $\mathbf{x}^\dagger \in \mathcal{X}^\dagger$. If TALYS is run twice for some $\mathbf{x}$, the two outputs $f^{(1)}(\mathbf{x})$ and $f^{(2)}(\mathbf{x})$ will be the same. Because TALYS is deterministic,  $f^{(1)}(\mathbf{x}) = f^{(2)}(\mathbf{x})~ \forall~ \mathbf{x}$. Consequently the uncertainty around $f(\mathbf{x})$ reduces to zero once it has been observed^[provided that, as is the case with TALYS, the simulator is deterministic and consequently there is no uncertainty due to random behaviour, often called aleatoric uncertainty, associated with the process $f$.]. However, for $f(\mathbf{x}^\dagger)$ where the output has not been observed (the simulator has not been run at $\mathbf{x^\dagger}$), there is uncertainty about its output due to incomplete knowledge about the process. The models and their implementation are understood, but not well enough that it is known what they will predict for a given $\mathbf{x}$ without running the simulator. This uncertainty is often described as epistemic, and in the Bayesian framework, statistical models are used to describe our belief about unobserved quantities, and the uncertainty associated with that belief. Consequently, the outputs $f(\mathcal{X^\dagger})$ are modelled as random variables. It is assume that $f$ is the sum of a deterministic trend function, $\mu(\mathbf{\cdot})$ and a zero mean Gaussian process, $Z(\mathbf{\cdot})$ [@dicekriging]:

\begin{equation}
(\#eq:process)
f(\mathbf{x}) = \mu(\mathbf{x}) + Z(\mathbf{x}).
\end{equation}

A Gaussian process (GP) is an infinite collection of random variables, any finite number of which has a multivariate Gaussian distribution [@gp4ml]. In practice the interest lies with two subsets of all possible $f(\mathbf{x})$, the observed simulator outputs $f(\mathcal{X^*})$, and the outputs we wish to predict $f(\mathcal{X^\dagger})$. Before any simulator outputs are observed, our belief about the joint distribution of the observed and unobserved outputs is modelled as

\begin{equation}
(\#eq:joint-distro)
\begin{bmatrix} \mathbf{f(\mathcal{X}^*)} \\
\mathbf{f(\mathcal{X}^\dagger)}
\end{bmatrix}
\sim MVN\left( \begin{bmatrix} \mu(\mathcal{X^*})\\
\mu(\mathcal{X^\dagger}) \end{bmatrix},
 \begin{bmatrix} \mathbf{\Sigma^*} & \mathbf{\Sigma^{*\dagger}}\\
 \mathbf{\Sigma^{\dagger*}} & \mathbf{\Sigma^{ \dagger}} \end{bmatrix} \right)
\end{equation}

For some mean function $\mu(\cdot)$ and some covariance matrices $\mathbf{\Sigma^*}, \mathbf{\Sigma^\dagger}$, which are functions of the $\mathcal{X^*}$ and $\mathcal{X^\dagger}$ respectively, and $\mathbf{\Sigma^{*\dagger}} = (\mathbf{\Sigma^{ \dagger *}})^{-1}$, which are functions of both $\mathcal{X^*}$ and $\mathcal{X^\dagger}$.

The function $Z(\cdot)$ is constrained to have specific properties in order to qualify as a GP, as the $\Sigma$ must have the similar fundamental properties as the covariance matrix of a multivariate normal distribution (Section \@ref(covariance)). The function $\mu(\cdot)$ can be any normal linear regression function (Section \@ref(mean)).

Both $\mu(\cdot)$ and $Z(\cdot)$ are functions with parameters that must be learn about from the $(\mathcal{X^*}, f(\mathcal{X^*}))$. Parameter estimation methods vary between software implementations (Section \@ref(gp-software)). After learning the parameters, the distribution in Equation \@ref(eq:joint-distro) is fully characterised for a given $\mathcal{X^\dagger}$, and the outputs at these unknown points are emulated as the joint conditional distribution $\mathbf{f}(\mathcal{X^\dagger})|\mathbf{f}(\mathcal{X^*})$.


## Parameter estimation {#model-selection}

The mean function in a GP takes the form of a linear model

\begin{equation}
(\#eq:mean-function)
\mu(\mathbf{x}) = \sum_{i=1}^b h_b(\mathbf{x})^T \mathbf{\beta}_b
\end{equation}

Where $b$ is the number of basis functions in the model. These basis functions are chosen by the user. Although any linear combination of basis functions is permissible, some software implementations offer more flexibility than others (Section \@ref(gp-software)). The $\mathbf{\beta}_b$ must be learned from the data. 

Each element of the covariance matrix in Equation \@ref(eq:joint-distro) is computed by using some function  $c(\mathbf{x}, \mathbf{x'})=\sigma^2 r(\mathbf{x}, \mathbf{x'})$, where $r$ is a correlation function that depends on $|\mathbf{x} - \mathbf{x'}|$ where $\mathbf{x}$ and $\mathbf{x'}$ are two points from $\mathcal{X}$. So the covariance between two of the $f$s depends in some way on the distance between their $\mathbf{x}$-coordinates. The function $r$ has $p$ hyperparameters $\mathbf{\gamma}_1,\mathbf{\gamma}_2,...,\mathbf{\gamma}_p$, one for each element of $\mathbf{x}$, which must be learned from the observed data, along with the variance parameter $\theta$. An optional noise parameter, sometimes called a nugget, can be learned (see Section \@ref(covariance) for more on the $\mathbf{\gamma}$ and the noise parameter). The vector of model parameters is denoted $\mathbf{\theta}$.

The mean function can simply be $\mu(\mathbf{x})=0~\forall~\mathbf{x}$ and as such the minimum amount of parameters that must be learned from the data for a function with a $p-$dimensional input is $p+1$, $p$ $\gamma$s and one $\sigma$. In a Bayesian context an expression proportional to the joint posterior distribution of the the parameters $\mathbf{\theta}$ given the observed data^[The observed data includes both the outputs $f(\mathcal{X^*})$ and the inputs $\mathcal{X}^*$ but the inputs are omitted here for brevity.] $f(\mathcal{X^*})$ is derived $\pi(\mathbf{\theta}|f(\mathcal{X^*}))$ using the  product of the likelihood of the data given the parameters $L(f(\mathcal{X^*})| \mathbf{\theta})$ and the priors for the parameters $\pi(\mathbf{\theta})$. One approach is to then use MCMC to generate samples from $\pi(\mathbf{\theta}|f(\mathbf{\mathbf{x^*}}))$. However this requires a great many proposal samples from the posterior to be generated. Consequently if it is computationally expensive to generate one proposal sample, this method becomes impractical, and an alternative approach is required. A common compromise is to find the mode of the posterior distributions and use these modal values as point estimates for the parameters in the model.

In this dissertation the Dice Kriging [@dicekriging] software package was used, which implements improper uniform priors [@dicekriging] for the $\mathbf{\theta}$ and as a result $\pi(\mathbf{\theta}|f(\mathcal{X^*})) \propto L(f(\mathcal{X^*})| \mathbf{\theta})$. Consequently ML estimation was used to learn the $\mathbf{\theta}$, which is equivalent to finding the maximum posterior mode in the uniform prior case, provided the domain of the likelihood is the same as, or a subset of, the domain of the priors. The training points  $f(\mathcal{X^*})$ are assumed to have a joint Gaussian distribution with mean $\mu(\mathcal{X^*})=\mathbf{h(\mathcal{X^*})\mathbf{\beta}}$ and covariance $\mathbf{\Sigma^*}=\mathbf{C} = \sigma^2 \mathbf{R}$, where $\mathbf{R}$ is a $k \times k$ covariance matrix with the $i,j$-th element equal to $r(\mathbf{x_i}, \mathbf{x_j})$ and where $k$ is the number of training points and^[Indices $i$ and $j$ are borrowed  here and not again; elsewhere i indexes over processes/measurements/emulator outputs and j indexes over proposal points from $\mathcal{X}$.] $i \in 1,2,...,k$, $j \in 1,2,...,k$. Hence the likelihood function that is maximised is

\begin{equation}
(\#eq:likelihood)
L(f(\mathcal{X^*})| \mathbf{\theta}) = \frac{1}{(2\pi)^{k/2} |\sigma^2 \mathbf{R}|^{1/2}} \exp \left( - \frac{1}{2} \left( f(\mathcal{X^*}) - \mathbf{H}^T\mathbf{\beta}\right)^T \left(\sigma^2 \mathbf{R} \right)^{-1} \left( f(\mathcal{X^*}) - \mathbf{H}^T\mathbf{\beta}\right) \right)
\end{equation}

 Where $\mathbf{H}$ is the $k \times p$ matrix with each row $\mu(\mathbf{\mathbf{x}})$ for one of the $k$ training points and each column one of the $p$ parameters. Each time the likelihood is evaluated, the matrix $\mathbf{R}$ needs to be inverted, which requires $\mathcal{O}(k^3)$ computation^[The computational expense is proportional to $n^3$] [@gelman2013bayesian], which means the computational expense can become prohibitively large if the likelihood has to be evaluated a great many times and/ or for a great many training points. This, and numerical issues that can arise from inverting $\mathbf{R}$ (see Section \@ref(covariance)) are the two main hurdles that motivate the use of posterior mode/ ML point estimates of the model parameters, as opposed to 'full Bayesian' distributions, which usually requires the likelihood to be evaluated many more times.

 Equation \@ref(eq:likelihood) is maximised with respect to the $\mathbf{\beta}, \sigma$ and $\mathbf{\gamma}$. Analytic expressions for ML estimators for $\mathbf{\beta}$ and $\sigma$ can be found in terms of $\mathbf{R}$ and the data:

\begin{equation}
(\#eq:beta-hat)
\mathbf{\hat{\beta}} = \left( \mathbf{H}^T\mathbf{R}^{-1}\mathbf{H} \right)^{-1}\mathbf{H}^T \mathbf{R}f(\mathcal{X^*})
\end{equation}

and

\begin{equation}
(\#eq:sigma-hat)
\hat{\sigma}^2 = \frac{1}{k} \left( f(\mathcal{X^*}) - \mathbf{H}^T\mathbf{\hat{\beta}}\right)^T \mathbf{R} ^{-1}\left( f(\mathcal{X^*}) - \mathbf{H}^T\mathbf{\hat{\beta}}\right)
\end{equation}

which look similar to the predictors for the similar terms in ordinary linear model theory, except for the presence of $\mathbf{R}$ which accounts for correlation in the residuals. With this re-parametrisation, the likelihood need only be maximised with respect to the $\mathbf{\gamma}$, a slightly easier numerical task. The maximum of the likelihood is then found using a gradient-based numerical optimisation algorithm, with multiple starting points to combat possible multi-modality in the likelihood [@dicekriging]. Once the parameter estimates are obtained the joint distribution of $f(\mathcal{X^*})$ and $f(\mathcal{X^\dagger})$ is fully characterised. Consequently the joint conditional distribution $f(\mathcal{X^\dagger})|f(\mathcal{X^*}), \mathbf{\theta}$ can be found, which is used to emulate the simulator at unobserved $\mathbf{x}$.

## Conditioning on the data

Assume that $f(\mathcal{X^*})$ and $f(\mathcal{X^\dagger})$ in Equation \@ref(eq:joint-distro) each have length one. Once estimates for the parameters $\mathbf{\hat{\theta}}$ have been computed, their joint distribution would look something like Figure \@ref(fig:conditioning), which shows a bivariate Gaussian distribution where the ovals are lines of constant probability density. There is some positive correlation between these two simulator outputs; perhaps because their $\mathbf{x}$-coordinates are close together. The left hand plot represents our prior belief about $f$ evaluated for any two active parameter vectors $\mathbf{x_1}$ and $\mathbf{x_2}$ before the simulator is run at thes points, and can be founding  by plugging $\mathbf{x_1}$ and $\mathbf{x_2}$ into the mean and covariance functions discussed in Sections \@ref(covariance) and \@ref(mean). The right hand plot shows what happens when the simulator output at $f(\mathbf{x_1})$ is observed. Our belief about the simulator output at $\mathbf{x_2}$ changes. The joint bivariate density is three-dimensional, and one can imagine slicing it with the plane at $f(\mathbf{x_1}) =f(\mathbf{x^*_1})$ and observing the exposed surface, whose two dimensions are  the $f(\mathbf{x_2})$ axis and a univariate Gaussian density. This is the conditional density of $f(\mathbf{x_2})|f(\mathbf{x^*_1})$. For a single unevaluated simulator output at $\mathbf{x^\dagger}$ with observed simulator outputs $f(\mathcal{X^*})$ the conditional mean, $E[f(\mathbf{x^\dagger})]$  and variance, $V[f(\mathbf{x^\dagger})]$ are given by

\begin{equation}
(\#eq:pred-mean)
 E[f(\mathbf{x^\dagger})]= \mathbf{h}(\mathbf{x^\dagger})^T \hat{\mathbf{\beta}} + \mathbf{c}(\mathbf{x}^\dagger) \mathbf{C}^{-1} \left( f(\mathcal{X^*}) - \mathbf{h}(\mathbf{x})^T \hat{\mathbf{\beta}} \right)
\end{equation}

and

\begin{align}
(\#eq:pred-var)
V[f(\mathbf{x^\dagger})] = c(\mathbf{x^*}, \mathbf{x^*}) - c^T(\mathbf{x^*})\mathbf{C}^{-1} c(\mathbf{x^*}) +\\ \nonumber
\left( \mathbf{h}(\mathbf{x^\dagger}) - \mathbf{h}^T(\mathbf{x^*}) \mathbf{C}^{-1} \mathbf{c}(\mathbf{x^\dagger}) \right)^T \left( \mathbf{h}^T(\mathbf{\mathbf{x^*}}) \mathbf{C}^{-1} \mathbf{h}(\mathbf{x^*}) \right)^{-1} \left( \mathbf{h}(\mathbf{x^\dagger}) - \mathbf{h}^T \mathbf(x^\dagger) \mathbf{C}^{-1} \mathbf{c}(\mathbf{x^\dagger}) \right)
\end{align}

```{r conditioning, echo=FALSE, fig.cap="Projection of the joint distribution of a simulator output for two active parameter settings onto a plane. The ovals are lines of constant probability density. Once one of the outputs is observed, the other has a univariate Gaussian distribution, conditional on the observation."}
knitr::include_graphics("~/Maths/Sheffield/dissertation/BookdownTemplate/Dissertation/figures/conditioning_thicker.png", error=T,rel_path=FALSE)
```

where $\mathbf{c}(\mathbf{x^\dagger})$ denotes the vector $\left( c(\mathbf{x^\dagger}, \mathbf{x_1^*}),c(\mathbf{x^\dagger}, \mathbf{x_1^*}),...,c(\mathbf{x^\dagger}, \mathbf{x_k^*}) \right)$ where $k$ is the number of observed training points. So $c(\mathbf{x^\dagger})$ is the vector of covariances between $\mathbf{x^\dagger}$ and each of the $k$ observed points. Chapter 2 of [@jeothesis] shows how these equations arise from properties of multivariate normal distributions.

If Equations \@ref(eq:pred-mean) and \@ref(eq:pred-var) are used to predict the simulator output at an observed point $\mathbf{x^*_d}$ ($d \in 1,2,...,k$) from one of the training runs then $\mathbf{c}^T(\mathbf{x_d}) \mathbf{C}^{(-1)} = \mathbf{C}^{(-1)} \mathbf{c}(\mathbf{x_d}) = \mathbf{e_d}$ where $\mathbf{e_d}$ is the k-vector where the d-th element is one and all the other elements are zero. This arises from properties of the covarince function described in Section \@ref(covariance). Consequently

\begin{align}
(\#eq:pred-design-1)
E[f(\mathbf{x^*_d})] &= \mathbf{h}(\mathbf{x^*_d})^T \hat{\mathbf{\beta}} + f(\mathbf{x^*_d}) - \mathbf{h}(\mathbf{x^^*_d})^T \hat{\mathbf{\beta}} \\ \nonumber
&= f(\mathbf{x^*_d})
\end{align}

and

\begin{align}
(\#eq:pred-design-2)
V[f(\mathbf{x^*_d})]  &= c(\mathbf{x_j}, \mathbf{x_j}) - c(\mathbf{x_j}, \mathbf{x^*_d}) + \left( \mathbf{h}(\mathbf{x^*}) - \mathbf{h}(\mathbf{x^*}) \right)\mathbf{h}(\mathbf{x^*})\left( \mathbf{h}(\mathbf{x^*}) - \mathbf{h}(\mathbf{x^*}) \right)\\ \nonumber
&= 0
\end{align}

and as such the observed $f(\mathcal{X^*})$ are interpolated exactly with zero uncertainty. This is desirable in cases where the data are observed with no error, as is the case with a deterministic computer simulation. Intuitively we would expect this to be true as Equations \@ref(eq:pred-mean) and \@ref(eq:pred-var) define the conditional distribution of $f(\mathbf{x}^\dagger)|f(\mathcal{X^*})$, and if $f(\mathbf{x}^\dagger)=f(\mathbf{x*})$ then the distribution of $f(\mathbf{x^*})|f(\mathcal{X^*})$ is just $f(\mathbf{x*})$. Stochastic behaviour in the function being emulated can also be encapsulated by estimating a 'nugget' parameter form the data, which is discussed next, along with other properties of the covariance function.

## Covariance function {#covariance}

The covariance between $f(\mathbf{x})$ and $f(\mathbf{x'})$ is computed from the parameter vectors $\mathbf{x}$ and $\mathbf{x'}$. A key assumption of Gaussian processes is that points that are close together in $\mathbf{x}$-space should be close together in $f(\mathbf{x})$-space, and that when predicting $f(\mathbf{x^\dagger})$, then observed points close to $\mathbf{x^\dagger}$ more strongly influence the prediction than those that are far away. The presence of the $\mathbf{x}-\mathbf{x}'$ term in correlation functions encapsulates this idea. Another key assumption is that some elements of $\mathbf{x}$ bear more influence on neighbouring points than others. and that information on this can be found from the data. The hyperparameters $\mathbf{\gamma}$ of the function $c(\cdot, \cdot)$ encapsulate this idea. The function $c(\cdot, \cdot)$ must
produce covariances matrices for $f$ that have several properties:

* Covariance matrices are symmetric in that for matrix $\mathbf{A}$, $\mathbf{A}=\mathbf{A}^{-1}$. Consequently the covariance function $c(\cdot, \cdot)$ must also be symmetric in that $c(\mathbf{x}, \mathbf{x'}) = \cdot(\mathbf{x'}, \mathbf{x})$.
* Covariance matrices are positive semidefinite (PSD), which means they have no negative eigenvalues. Hence $c(\cdot,\cdot)$ must produce covariance matrices which are PSD.
* A covariance function that is a function of $\mathbf{x} - \mathbf{x'}$ is stationary, in that the covariance between two points does not depend on where they are in $\mathcal{X}$ but only the distance between them.
* Furthermore a covariance function that is a function of $|\mathbf{x} - \mathbf{x'}|$ is isotropic.

If a covariance function is stationary, then it can be expressed as a product of a correlation matrix $\mathbf{R}$ and a constant variance $\sigma^2$. Table \@ref(tab:kernels) shows the univariate version of the covariance functions available in DiceKriging [@dicekriging], or more appropriately correlation functions, as each is multiplied by $\sigma$ to get the required covariance. All of the covariance functions in Table \@ref(tab:kernels) produce symmetric, PSD matrices, and they are all stationary and isotropic.  

\begin{table}\centering
\caption{Correlation functions availabe in the software package DiceKriging.}
\label{tab:kernels}
\begin{tabular}{|l|l|}
\hline
\textbf{Name} & \textbf{Expression} \\
\hline
Gaussian & $\exp(- \frac{(x - x')^2}{2\gamma^2})$\\
Matérn  $\nu = \frac{5}{2}$ & $\left( 1 + \frac{\sqrt{5}|(x - x')|}{\gamma} + \frac{5(x - x')^2}{3\gamma^2}\right)\exp\left(-\frac{\sqrt{5}|(x - x')|}{\gamma} \right)$ \\
Matérn $\nu = \frac{3}{2}$ &$\left( 1 + \frac{\sqrt{3}|(x - x')|}{\gamma}\right)\exp(-\frac{\sqrt{3}|(x - x')|}{\gamma})$\\
Exponential & $\exp\left(-\frac{|(x - x')|}{\gamma} \right)$\\
Power-Exponential & $\exp\left(-\left(\frac{|(x - x')|}{\gamma}\right)^p\right)$\\
\hline
\end{tabular}
\end{table}

The Matérn correlation function with parameter $\nu = \frac{5}{2}$ is a popular choice and is the default option in both the DiceKriging and RobustGaSP [@rgasp_manual] software packages. The $\nu$ parameter in the Matérn family controls the smoothness of the function. As $\nu \rightarrow \infty$, the Matérn function begins to look more like the Gaussian covariance function, which is infinitely differentiable and can hence be unrealistically smooth. For $\nu = \frac{5}{2}$, the process is mean  square twice differentiable, which guarantees the existence of first and second moments, and hence predictive means and variances, which is desirable. Achieving the same level of smoothness with the squared exponential kernel would come at the price of correlations quickly going to zero as $|\mathbf{x} - \mathbf{x'}|$ increases [@rgasp_manual], which may be unrealistic, and cause numerical issues in the parameter estimation routine. In this dissertation, the Matérn $\frac{5}{2}$ correlation function is used.

The correlation functions $r(\cdot, \cdot)$ in Table \@ref(tab:kernels) take two scalar inputs and return a single scalar output^[A function that does this is called a kernel, and hence the $r(\cdot, \cdot)$ are sometimes called kernels.]. To extend this to compute correlations for multivariate $\mathbf{x}$ the most common approach is to take the product of the univariate correlations:

\begin{equation}
(\#eq:mv-correlation)
\mathbf{c}(\mathbf{\mathbf{x}, \mathbf{x'}}) = \sigma^2 \prod_{i=1}^p c(x_i, x'_i)
\end{equation}

where $p$ is the number of elements of $\mathbf{x}$. For each of element of $\mathbf{x}$ a separate $\gamma$ is learned, and that controls how influential that input parameter is on determining the value and the precision of the emulator prediction at other points, by inflating or shrinking the effective distance between the points in the relevant $\mathbf{x}$-component. The $\mathbf{\gamma}$ are often called length-scale parameters. For predictions at points in $\mathcal{X}$ further away from the observed $\mathbf{x^*}$, the predictive variance will tend to $\sigma^2$, as the $(x-x')$ terms in the correlation functions go to infinity, and as such the exponential terms in Table \@ref(tab:kernels) tend to one. If $\gamma$ is changed from 1 to 0.5 for one of the components of $\mathbf{x}$, the average amount of variation observed in a fixed interval along the component axis would double. Their is an interplay between the $\mathbf{\gamma}$ and $\sigma$, The smaller the $\mathbf{\gamma}$ are the more the variation arises due to the function, and hence $\sigma^2$ would be smaller, as large fluctuations in $f$ can be explained by function variation rather than noise. Higher values for $\mathbf{\gamma}$ are associated with higher values for $\sigma^2$ and imply a constant function with a lot noise. Lower values for $\mathbf{\gamma}$ are associated with lower values for $\sigma^2$ and imply a white-noise process [@gp4ml].

Often the function being emulated is non-deterministic. In this case, it is possible to learn another parameter from that data, called the 'nugget', which is the variance of a zero-centred Gaussian distribution. The univariate nugget is added to the diagonal elements of the covariance matrix for the observed data and the emulator is no longer constrained to interpolate the observed $f(\mathcal{X^*})$. The nugget can also be used to address problems with inverting $\mathbf{R}$ in Equation \@ref(eq:likelihood) which can arise when its elements are very close to 0 or 1. In this dissertation a nugget parameter was used when training emulators to help with likelihood estimation.

There is a lot of flexibility in choosing covariance functions and they can be customised to reflect prior knowledge of the function being emulated. In particular, any sum or product of valid covariance functions is also a valid covariance function
[@gp4ml]. The covariance function is the defining element of a GP, but emulation is often boosted by the inclusion of a mean function, which is illustrates with a simple example, along with some other properties of the GP discussed in this section.

```{r gp_example_data, include=FALSE,echo=FALSE,warning=FALSE,message=FALSE}
all_x <- seq(from=-10,to=10,length.out=100)
all_y <- all_x^2 - 5*all_x
extend_x <- seq(from=-10,to=100,length.out=1000)
subset <- readr::read_rds("~/Maths/Sheffield/dissertation/BookdownTemplate/Dissertation/data/subset.rds")
data <- tibble::tibble(all_x,all_y) %>%
  dplyr::rename("x" = all_x, "y" = all_y)

mod <- RobustGaSP::rgasp(design = as.matrix(subset$x), response = subset$y,
                zero.mean = "No",
                nugget.est =F )
p <-  predict(mod, all_x, X = mod@X)
preds_mod <- tibble::tibble(mean = p$mean, lower95 = p$lower95,
                            upper95 = p$upper95, x= all_x) %>%
  dplyr::left_join(subset,by="x") %>% 
  dplyr::mutate(Model = "Constant mean")
mod_zero <- RobustGaSP::rgasp(design = as.matrix(subset$x), response = subset$y,
                       zero.mean = "Yes",
                       nugget.est = F)
p_zero <-  predict(mod_zero, all_x, X = mod_zero@X)
preds_mod_zero <- tibble::tibble(mean = p_zero$mean, lower95 = p_zero$lower95,
                            upper95 = p_zero$upper95, x= all_x) %>%
  dplyr::left_join(subset,by="x") %>%
  dplyr::mutate(Model = "Zero-mean")

long_x <- seq(from=-10,to=100,length.out=1000)
p <-  predict(mod, long_x, X = mod@X)
preds_mod_long <- tibble::tibble(mean = p$mean, lower95 = p$lower95,
                            upper95 = p$upper95, x= long_x) %>%
  dplyr::left_join(subset,by="x") %>% 
  dplyr::mutate(Model = "Constant mean")
p_zero <-  predict(mod_zero, long_x, X = mod_zero@X)
preds_mod_zero_long <- tibble::tibble(mean = p_zero$mean, lower95 = p_zero$lower95,
                            upper95 = p_zero$upper95, x= long_x) %>%
  dplyr::left_join(subset,by="x") %>%
  dplyr::mutate(Model = "Zero-mean")

```
## Mean function {#mean}


A GP models our belief about how far a true process might deviate from some given regression function. Any variation in the true process not captured by the regression function needs to be captured by the GP, which can often lead to less precision in emulator predictions. The following example gives an illustration of this.

Assume the simulator $f(x)=x^2 - 5x$ from Section \@ref(hm-worked-example) and shown in Figure \@ref(fig:function-plot) is to be emulated using a GP and that the simulator can only be run at the five points shown. There is a region with no observed points, roughly between -9 and 0 on the x-axis. The emulator  will have to interpolate in this region. There are no observations in the region above about 6 on the x-axis, the emulator will have to extrapolate in this region. There is a good amount of data around the point of inflection at $x=2.5$. This is where the function varies the most.

```{r function-plot, echo=FALSE, fig.cap="A simple function to be emulated, with the five points used to build the emulator shown. There is a region with no training data from approximately x=-9 to x=0."}
subset %>% dplyr::rename("Samples" = y) %>% 
  dplyr::right_join(data, b = "x") %>%
  ggplot2::ggplot(ggplot2::aes(x=x,y=y)) +
  ggplot2::geom_line() + ggplot2::theme_bw() +
  ggplot2::geom_point(ggplot2::aes(y = Samples),col= "blue",size=2)
```

First a zero-mean GP emulator is built with a Matérn $\frac{5}{2}$ correlation function, without a 'nugget'. The emulator is then used to predict, with 95% posterior predictive intervals, the value of the function at many points between x = -10 and 10. A second emulator is built using a simple first-order^[as in $y=mx+c$] mean function, which requires two more parameters to be estimated from the data. The results are shown in Figure \@ref(fig:predictors). The predictive means of both emulators do a good job of reproducing the function and both emulators have larger predictive intervals further away from the data. However, the constant-mean emulator (right pane) shows greater precision in predictions, because some part of the function variation is encapsulated by the mean function.

```{r predictors,echo=FALSE, fig.cap="Predictions using a zero-mean (left) and first-order mean (right) GP emulator trained on the five points shown for the function, with 95 percent predictive intervals shown. The emulators predict the training points exactly, and the width of the predictive intervals increases further from the data. The first-order mean emulator is able to interpolate and extrapolate with greater precision.",fig.height=6.5,fig.width=6.5}

p1 <- preds_mod_zero%>% ggplot(aes(x=x,y = mean)) + geom_line() +
  geom_line(aes(y=upper95), color="red") +
  geom_line(aes(y=lower95), color="red") +
  geom_point(aes(y=y),col="blue") +
  theme_bw() + ylab("f(x)") +
  ggplot2::theme(axis.text = ggplot2::element_text(size=13),
                 axis.title = ggplot2::element_text(size=15))
p2 <- preds_mod %>% ggplot(aes(x=x,y = mean)) + geom_line() +
  geom_line(aes(y=upper95), color="red") +
  geom_line(aes(y=lower95), color="red") +
  geom_point(aes(y=y),col="blue") +
  theme_bw() + theme(axis.title.y = element_blank()) +
    ggplot2::theme(axis.text = ggplot2::element_text(size=13),
                 axis.title = ggplot2::element_text(size=15))
gridExtra::grid.arrange(p1,p2,ncol=2)
```

As the emulator tries to predict at points further away from the training data, the influence of the training data on the predictions drops off, and the emulator reverts to the mean function, as show in Figure \@ref(fig:predict-long), where the zero-mean emulator on the left tends to zero with larger x, and the constant-mean emulator on the right tends to the mean function, albeit both with such wide posterior predictive intervals as to render the results quite useless.

```{r predict-long, fig.cap="Behaviour of the emulator as it predicts further from the training data. Both the zero-mean (left) and first-order mean (right) emulators tend to their respective mean functions as the distance from the observed data grows larger.", fig.width=6.5,fig.height=6.5}
p1 <- preds_mod_zero_long%>% ggplot(aes(x=x,y = mean)) + geom_line() +
  geom_line(aes(y=upper95), color="red") +
  geom_line(aes(y=lower95), color="red") +
  geom_point(aes(y=y)) +
  theme_bw() + ylab("f(x)") +
    ggplot2::ylim(c(-700,700))
p2 <- preds_mod_long %>% ggplot(aes(x=x,y = mean)) + geom_line() +
  geom_line(aes(y=upper95), color="red") +
  geom_line(aes(y=lower95), color="red") +
  geom_point(aes(y=y)) +
  theme_bw() + theme(axis.title.y = element_blank())+ 
  ggplot2::ylim(c(-700,700))
gridExtra::grid.arrange(p1,p2,ncol=2)
```


Theoretically, the mean function can be any linear (in the coefficients) function of the inputs $\mathbf{x}$. Some sources [@gp_comparison] favour a first order-only model, describing the estimation of coefficients for higher order polynomials as possibly extraneous. Prior knowledge of the process to be emulated can be useful in choosing a suitable mean function. In this dissertation, where, 28 input parameters were being varied, and knowledge of the relationship between them and the simulator output was limited, a mean function of the kind

\begin{equation}
(\#eq:simple-mean-function)
\mu(\mathbf{x}) = \beta_0 + \sum_{i=1}^{28} \beta_i x_i
\end{equation}

was used, which meant that 29 parameters needed to be learned for the mean function. This was a practical choice - each emulator was trained using several hundred runs. Introducing second order terms may have required many more simulator runs to get good reliable estimates for the $\mathbf{\beta}$, and the computational resource required for this was not available. 
The choice of mean function, and other modelling choices, can also be a function of the software package used, and we discuss the choice of software implementation in the next section.

## Model complexity and the predictive distribution

The denominator in Equation \@ref(eq:sigma-hat) should be $k-p$, where $p$ is the number of parameters in the GP mean function (Section \@ref(mean)), and consequently the predictive distribution of the GP emulator is a Student-t with $k-p$ degrees of freedom. Often in practice, as is the case with the software used in this dissertation, it is assumed that $k>>p$ and as such the number of degrees of freedom is large enough that the predictive distribution is approximately Normal. In what follows it is assumed that the predictive distributions of the emulators are Gaussian, as the number of simulator runs used to train the emulators numbered in the hundreds, and the number of parameters was in the tens. Care must be taken with model complexity, if $k$ is not much greater than $p$, an assumption of Gaussian predictions may not be valid, and the predictive intervals significantly wider than assumed.

## Software implementations {#gp-software}

Three software packages were trialled for the GP emulation in this dissertation, the R package RobustGaSP [@rgasp_manual], the Python module scikit-learn [@scikit] GaussianProcessRegressor, and the R package DiceKriging [@dicekriging]. 

RobustGaSP package distinguishes itself from the other two by implementing a non-uniform prior for the GP parameters and finding their maximum posterior modes as point estimates. This approach is supposed to protect against some of the numerical issues arising from parameter estimation (discussed briefly in Section \@ref(model-selection)). However, this software package was very slow to run, and as such it was impractical for training 588 emulators using hundreds of data points at a time. Crucially, it did not expose a public method for generating a predictive covariance matrix, which was required for model validation, as discussed in Section \@ref(wave1-methodology). 

GaussianProcessRegressor from scikit-learn was the only Python module examined. It was very quick, and appeared to offer the greatest flexibility in building custom covariance functions. However, it failed very often in parameter estimation, and it only offered two mean function options: zero-mean or constant (equal to the mean of the training data), which may have been the cause of its problem in parameter estimation. 

In this dissertation, 588 emulators were trained in batches for processes with unknown behaviours. As such, it was impractical to have to wait a long time for parameters to be estimated, or to spend much time examining the impacts of different covariance functions across hundreds off models. The preferred method was to build the emulators with sensible defaults, and to conservatively filter out models with poor generalisation performance. It was also crucial to be able to easily generate predictive covariance matrices. Consequently, the DiceKriging package was used, as it was relatively fast, and could compute covariance matrices using its 'predict' method.

The order of the information presented in this chapter was not indicative of the modelling process workflow, so the chapter finishes with a summary of GP modelling considerations, and a reminder of the purpose of GPs in this dissertation.

## Conclusion

In this chapter some of the theory and the main building blocks of GPs were presented. In summary, assuming adequate training data has been collected on the function to emulate:

1. Choose a structure for the mean and covariance function,
2. Compute the parameters for the emulator by maximising the likelihood of the data with respect to the parameters,
3. This gives a model for the joint distribution of the unobserved and observed points, from which the conditional distribution of the unobserved points is derived. It is this conditional distribution that is used to predict at the unobserved points.

The simulator output an unobserved $\mathbf{x}$ is a Gaussian random variable, which implies a point estimate for the function evaluated at that $\mathbf{x}$ (the mean) and a quantification of the uncertainty of that point estimate (the variance). These estimates are obtained in a fraction of the time it would take to run the simulator. These properties make  GP emulation a vital tool in history matching, which is discussed in the next section.
